{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import biom\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from typing import Callable\n",
    "from scipy.spatial import distance\n",
    "from skbio import (OrdinationResults,\n",
    "                   DistanceMatrix)\n",
    "from scipy.sparse.linalg import svds\n",
    "\n",
    "from gemelli.optspace import svd_sort\n",
    "from gemelli.ctf import ctf_table_processing\n",
    "from gemelli.preprocessing import (build_sparse,\n",
    "                                   matrix_rclr)\n",
    "from gemelli._defaults import (DEFAULT_COMP, DEFAULT_MSC,\n",
    "                               DEFAULT_MFC, DEFAULT_MFF,\n",
    "                               DEFAULT_TEMPTED_PC,\n",
    "                               DEFAULT_TEMPTED_EP,\n",
    "                               DEFAULT_TEMPTED_SMTH,\n",
    "                               DEFAULT_TEMPTED_RES,\n",
    "                               DEFAULT_TEMPTED_MAXITER,\n",
    "                               DEFAULT_TEMPTED_RH as DEFAULT_TRH,\n",
    "                               DEFAULT_TEMPTED_RHC as DEFAULT_RC,\n",
    "                               DEFAULT_TEMPTED_SVDC,\n",
    "                               DEFAULT_TEMPTED_SVDCN as DEFAULT_TSCN)\n",
    "\n",
    "from gemelli.tempted import (freg_rkhs, bernoulli_kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_time(individual_id_tables,\n",
    "                individual_id_state_orders, \n",
    "                n_individuals, resolution, \n",
    "                timestamps_all, interval=None):\n",
    "    '''\n",
    "    Normalize time points to be in the same format and keep\n",
    "    only the defined interval (if defined)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    individual_id_tables: dictionary, required\n",
    "        Dictionary of tables constructed.\n",
    "        (see build_sparse class)\n",
    "        keys = individual_ids\n",
    "        values = DataFrame, required\n",
    "            rows = features\n",
    "            columns = samples\n",
    "\n",
    "    individual_id_state_orders : dict\n",
    "        Dictionary of time points for each individual\n",
    "\n",
    "    n_individuals: int, required\n",
    "        Number of unique individuals/samples\n",
    "\n",
    "    resolution: int, optional : Default is 101\n",
    "        Number of time points to evaluate the value\n",
    "        of the temporal loading function.\n",
    "\n",
    "    interval : tuple, optional\n",
    "        Start and end time points to keep\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    ti: list of numpy.ndarray\n",
    "        List of time points within defined interval \n",
    "        per subject\n",
    "\n",
    "    ind_vec: numpy.ndarray\n",
    "        Subject indexes for each time point\n",
    "\n",
    "    tm: numpy.ndarray\n",
    "        Concatenated normalized time points for all\n",
    "        subjects\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    TODO\n",
    "    '''\n",
    "\n",
    "    # make copy of tables to update\n",
    "    tables_update = copy.deepcopy(individual_id_tables)\n",
    "    orders_update = copy.deepcopy(individual_id_state_orders)\n",
    "    # set the interval if none is given\n",
    "    if interval is None:\n",
    "        interval = (timestamps_all[0], timestamps_all[-1])\n",
    "    # set time ranges [0, 1]\n",
    "    input_time_range = (timestamps_all[0], timestamps_all[-1])\n",
    "    # normalize time points\n",
    "    for individual_id in orders_update.keys():\n",
    "        orders_update[individual_id] = (orders_update[individual_id] - input_time_range[0]) \\\n",
    "                                        / (input_time_range[1] - input_time_range[0])\n",
    "    # ensure interval is in the same format\n",
    "    interval = tuple((interval - input_time_range[0]) \\\n",
    "                     / (input_time_range[1] - input_time_range[0]))\n",
    "    \n",
    "    # initialize variables to store time points (tps)\n",
    "    Lt = [] # all normalized tps\n",
    "    ind_vec = [] #individual indexes for each tp\n",
    "    ti = [[] for i in range(n_individuals)] # tps within interval per subject\n",
    "    \n",
    "    # populate variables above\n",
    "    for i, (id_, time_range_i) in enumerate(orders_update.items()):\n",
    "        # save all normalized time points\n",
    "        Lt.append(time_range_i)\n",
    "        ind_vec.extend([i] * len(Lt[-1]))\n",
    "        # define time points within interval\n",
    "        mask = (time_range_i >= interval[0]) & (time_range_i <= interval[1])\n",
    "        temp = time_range_i[mask]\n",
    "        temp = [(resolution-1)*(tp - interval[0])/(interval[1] - interval[0]) for tp in temp]\n",
    "        ti[i] = np.array(list(map(int, temp)))\n",
    "        # update tables and orders\n",
    "        tables_update[id_] = tables_update[id_].T[mask].T\n",
    "    \n",
    "    # convert variables to numpy arrays \n",
    "    ind_vec = np.array(ind_vec)\n",
    "    tm = np.concatenate(Lt)\n",
    "\n",
    "    return interval, tables_update, ti, ind_vec, tm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_tabular(individual_id_tables, \n",
    "                       n_individuals,\n",
    "                       n_components=3):\n",
    "                   \n",
    "    \"\"\"\n",
    "    Initialize subject and feature loadings\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    individual_id_tables: dictionary, required\n",
    "        Dictionary of tables constructed.\n",
    "        (see build_sparse class)\n",
    "        keys = individual_ids\n",
    "        values = DataFrame, required\n",
    "            rows = features\n",
    "            columns = samples\n",
    "\n",
    "    n_individuals: int, required\n",
    "        Number of unique individuals/samples\n",
    "\n",
    "    n_components: int, optional : Default is 3\n",
    "        The underlying rank of the data and number of\n",
    "        output dimentions.\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    b_hat: dictionary\n",
    "        Updated feature loadings\n",
    "        keys = modality\n",
    "        values = loadings\n",
    "        \n",
    "    Raises\n",
    "    ----------\n",
    "    TODO\n",
    "    \"\"\"\n",
    "\n",
    "    # initialize feature loadings\n",
    "    data_unfold = np.hstack([m.values for m in individual_id_tables.values()])\n",
    "    u, e, v = svds(data_unfold, k=n_components, which='LM')\n",
    "    u, e, v = svd_sort(u, np.diag(e), v)\n",
    "    b_hat = u[:, 0]\n",
    "\n",
    "    # initialize subject loadings\n",
    "    consistent_sign = np.sign(np.sum(b_hat))\n",
    "    a_hat = (np.ones(n_individuals) / np.sqrt(n_individuals)) * consistent_sign\n",
    "\n",
    "    return b_hat, a_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_lambda(individual_id_tables, ti, \n",
    "                  a_hat, phi_hat, b_hat):\n",
    "    '''\n",
    "    Updates the singular values using the loadings\n",
    "    from the most recent iteration\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    individual_id_tables: dictionary, required\n",
    "        Dictionary of tables constructed. Note that at this point\n",
    "        the tables have been subset to only include the time points\n",
    "        within the previously defined interval.\n",
    "        keys = individual_ids\n",
    "        values = DataFrame, required\n",
    "            rows = features\n",
    "            columns = samples\n",
    "    ti: list of int, required\n",
    "        Time points within predefined interval for\n",
    "        each individual \n",
    "    a_hat: np.narray, required\n",
    "        Subject loadings from the previous iteration\n",
    "    phi_hats: np.narray, required\n",
    "        Temporal loadings from the previous iteration\n",
    "    b_hat: np.narray, required\n",
    "        Feature loadings from the previous iteration\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    lambda_new: dictionary\n",
    "        Updated singular values\n",
    "        keys = modality\n",
    "        values = loadings\n",
    "        \n",
    "    Raises\n",
    "    ----------\n",
    "    TODO\n",
    "    '''\n",
    "    \n",
    "    nums = []\n",
    "    denoms = []\n",
    "\n",
    "    for i, m in enumerate(individual_id_tables.values()):\n",
    "        \n",
    "        phi_ = phi_hat[ti[i]]\n",
    "        num = a_hat[i]*(b_hat.dot(m.values).dot(phi_))\n",
    "        nums.append(num)\n",
    "        denom = (a_hat[i]*phi_) ** 2\n",
    "        denom = np.sum(denom)\n",
    "        denoms.append(denom)\n",
    "    \n",
    "    lambda_new = np.sum(nums) / np.sum(denoms)\n",
    "\n",
    "    return lambda_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modality_iterator(individual_id_tables, \n",
    "                      individual_id_state_orders,\n",
    "                      mod_id_ind, interval, \n",
    "                      resolution=101, maxiter=20,\n",
    "                      epsilon=1e-4, smooth=1e-6,\n",
    "                      n_components=3):\n",
    "    '''\n",
    "    Iterate over the available modalities\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    individual_id_tables: dictionary, required\n",
    "        Dictionary of 1 to n tables constructed,\n",
    "        (see build_sparse class), where n is the \n",
    "        number of modalities.\n",
    "        keys = individual_ids\n",
    "        values = list of DataFrame, required\n",
    "            For each DataFrame (modality):\n",
    "            rows = features\n",
    "            columns = samples\n",
    "\n",
    "    individual_id_state_orders: dictionary, required\n",
    "        Dictionary of 1 to n lists of time points (one \n",
    "        per modality) for each sample.\n",
    "        keys = individual_ids\n",
    "        values = list of numpy.ndarray\n",
    "            Each numpy.ndarray contains the time points\n",
    "            of the corresponding modality\n",
    "            Note: array of dtype=object to allow for\n",
    "            different number of time points per modality\n",
    "\n",
    "    mod_id_ind: dictionary, required\n",
    "        Dictionary of individual IDs for each modality\n",
    "        keys = modality\n",
    "        values = list of tuples\n",
    "            Each tuple contains the individual id and \n",
    "            the dataframe index in individual_id_tables\n",
    "\n",
    "    interval : tuple, optional\n",
    "        Start and end time points to keep\n",
    "\n",
    "    resolution: int, optional : Default is 101\n",
    "        Number of time points to evaluate the value\n",
    "        of the temporal loading function.\n",
    "\n",
    "    maxiter: int, optional : Default is 20\n",
    "        Maximum number of iteration in for rank-1 calculation\n",
    "\n",
    "    epsilon: float, optional : Default is 0.0001\n",
    "        Convergence criteria for difference between iterations\n",
    "        for each rank-1 calculation.\n",
    "\n",
    "    smooth: float, optional : Default is 1e-6\n",
    "        Smoothing parameter for the kernel matrix\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    b_hats: dictionary\n",
    "        Updated feature loadings\n",
    "        keys = modality\n",
    "        values = loadings\n",
    "        \n",
    "    Raises\n",
    "    ----------\n",
    "    TODO\n",
    "    '''\n",
    "\n",
    "    # make copy of tables to update\n",
    "    orders_update = copy.deepcopy(individual_id_state_orders)\n",
    "    tables_update = copy.deepcopy(individual_id_tables)\n",
    "\n",
    "    #get all time points across all modalities\n",
    "    timestamps_all = np.concatenate(list(orders_update.values()))\n",
    "    timestamps_all = np.concatenate(timestamps_all)\n",
    "    timestamps_all = np.unique(timestamps_all)\n",
    "    \n",
    "    #initialize dictionary to store outputs per modality\n",
    "    #To-do: save in self\n",
    "    b_hats = {}\n",
    "    phi_hats = {}\n",
    "    a_hats = {}\n",
    "    lambdas = {}\n",
    "    table_mods = {}\n",
    "    times = {}\n",
    "    \n",
    "    #iterate through each modality\n",
    "    for modality in mod_id_ind.keys():        \n",
    "\n",
    "        #get the individual IDs\n",
    "        ind_tuple_lst = mod_id_ind[modality]\n",
    "        #keep modality-specific time points\n",
    "        orders_sub = {ind: orders_update[ind[0]][ind[1]] for ind in ind_tuple_lst}\n",
    "        #keep modality-specific tables\n",
    "        table_mod = {ind: tables_update[ind[0]][ind[1]] for ind in ind_tuple_lst}\n",
    "        n_individuals = len(table_mod)\n",
    "\n",
    "        #format time points\n",
    "        (norm_interval, table_mod, \n",
    "         ti, ind_vec, tm) = format_time(table_mod, orders_sub, \n",
    "                                        n_individuals, resolution,\n",
    "                                        timestamps_all, interval)        \n",
    "        #save key outputs\n",
    "        table_mods[modality] = table_mod\n",
    "        times[modality] = [ti, ind_vec]\n",
    "\n",
    "        #construct the kernel matrix\n",
    "        Kmat = bernoulli_kernel(tm, tm)\n",
    "        Kmat_output = bernoulli_kernel(np.linspace(norm_interval[0],\n",
    "                                                   norm_interval[1],\n",
    "                                                   num=resolution),\n",
    "                                       tm)\n",
    "        \n",
    "        #initialize feature and subject loadings\n",
    "        data_unfold = np.hstack([m.values for m in table_mod.values()])\n",
    "        b_hat, a_hat = initialize_tabular(data_unfold, \n",
    "                                          n_individuals=n_individuals,\n",
    "                                          n_components=n_components)\n",
    "        b_hats[modality] = b_hat\n",
    "        a_hats[modality] = a_hat\n",
    "\n",
    "    #TO-DO: update components for each modality\n",
    "    t = 0\n",
    "    dif = 1\n",
    "    while t <= maxiter and dif > epsilon:    \n",
    "        for modality in mod_id_ind.keys():\n",
    "            \n",
    "            #get key modality-specific variables\n",
    "            table_mod = table_mods[modality]\n",
    "            ti, ind_vec = times[modality]\n",
    "            a_hat = a_hats[modality]\n",
    "            b_hat = b_hats[modality]\n",
    "\n",
    "            #calculate state loadings\n",
    "            Ly = [a_hat[i] * b_hat.dot(m) for i, m in enumerate(table_mod.values())]\n",
    "            phi_hat = freg_rkhs(Ly, a_hat, ind_vec, Kmat, Kmat_output, smooth=smooth)\n",
    "            phi_hat = (phi_hat / np.sqrt(np.sum(phi_hat ** 2)))\n",
    "            phi_hats[modality] = phi_hat\n",
    "\n",
    "            #calculate lambda\n",
    "            lambda_mod = update_lambda(table_mod, ti, a_hat, phi_hat, b_hat)\n",
    "            lambdas[modality] = lambda_mod\n",
    "\n",
    "            #update tabular loadings\n",
    "            n_features = table_mod[0].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ind1': array([list([0, 0.5, 1, 2]), list([0, 0.5, 1]), list([0, 0.5, 1])],\n",
      "      dtype=object), 'ind2': array([list([0, 1, 3]), list([0, 0.5, 1, 3])], dtype=object)}\n",
      "{'ind1': array([0. , 0.5, 1. , 2. ]), 'ind2': array([0, 1, 3])}\n"
     ]
    }
   ],
   "source": [
    "#create dummy orders_update\n",
    "individual_id_state_orders = {'ind1': np.array([[0, 0.5, 1, 2],[0, 0.5, 1],[0, 0.5, 1]], dtype=object),\n",
    "                              'ind2': np.array([[0, 1, 3],[0, 0.5, 1, 3]], dtype=object)}\n",
    "\n",
    "individual_id_state_orders2 = {'ind1': np.array([0, 0.5, 1, 2]),\n",
    "                              'ind2': np.array([0, 1, 3])}\n",
    "\n",
    "print(individual_id_state_orders)\n",
    "print(individual_id_state_orders2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create random tables\n",
    "table1 = np.random.randint(0,100,size=(10, 5))\n",
    "table2 = np.random.randint(0,100,size=(12, 7))\n",
    "\n",
    "table3 = np.random.randint(0,100,size=(10, 6))\n",
    "table4 = np.random.randint(0,100,size=(12, 4))\n",
    "\n",
    "#create dictionary of tables\n",
    "individual_id_tables = {'ind1': [table1, table2], 'ind2': [table3, table4]}\n",
    "individual_id_mod = {'ind1': table1, 'ind2': table3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def udpate_tabular(individual_id_tables, \n",
    "                   n_individuals, a_hat,\n",
    "                   n_features, b_hat, \n",
    "                   ti, phi_hat):\n",
    "    '''\n",
    "    Update the tabular loadings (subjects and features) loadings\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    individual_id_tables: dictionary, required\n",
    "        Dictionary of tables constructed. Note that at this point\n",
    "        the tables have been subset to only include the time points\n",
    "        within the previously defined interval.\n",
    "        keys = individual_ids\n",
    "        values = DataFrame, required\n",
    "            rows = features\n",
    "            columns = samples\n",
    "    n_individuals: int, required\n",
    "        Number of unique individuals/samples\n",
    "    ti: list of int, required\n",
    "        Time points within predefined interval for\n",
    "        each individual \n",
    "    a_hat: np.narray, required\n",
    "        Subject loadings from the previous iteration\n",
    "    phi_hat: np.narray, required\n",
    "        Temporal loadings from the previous iteration\n",
    "    b_hat: np.narray, required\n",
    "        Feature loadings from the previous iteration\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    a_new: np.narray\n",
    "        Updated subject loadings\n",
    "    b_new: np.narray\n",
    "        Updated feature loadings\n",
    "        \n",
    "    Raises\n",
    "    ----------\n",
    "    TODO\n",
    "    '''\n",
    "    \n",
    "    # initialize variables\n",
    "    a_tilde = np.zeros(n_individuals)\n",
    "    temp_num = np.zeros((n_features, n_individuals))\n",
    "    temp_denom = np.zeros(n_individuals)\n",
    "    \n",
    "    #TO-DO: Iterate through modalities\n",
    "    for i, m in enumerate(individual_id_tables.values()):\n",
    "        \n",
    "        phi_ = phi_hat[ti[i]]\n",
    "        a_tilde[i] = b_hat.dot(m.values).dot(phi_)\n",
    "        a_tilde[i] = a_tilde[i] / np.sum(phi_ ** 2)\n",
    "        temp_num[:, i] = (m.values).dot(phi_)\n",
    "        temp_denom[i] = np.sum(phi_ ** 2)\n",
    "    \n",
    "    # update subject\n",
    "    a_new = a_tilde / np.sqrt(np.sum(a_tilde ** 2))\n",
    "    dif = np.sum((a_hat - a_new) ** 2)\n",
    "    a_hat = a_new\n",
    "    \n",
    "    # update feature loadings\n",
    "    b_tilde = temp_num.dot(a_hat) / (temp_denom.dot(a_hat ** 2))\n",
    "    b_new = b_tilde / np.sqrt(np.sum(b_tilde ** 2))\n",
    "    dif = max(dif, np.sum((b_hat - b_new) ** 2))\n",
    "    b_hat = b_new\n",
    "    t += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def udpate_residuals():\n",
    "    '''\n",
    "    Update the tensor to be factorized by subtracting the \n",
    "    approximation the previous iteration\n",
    "    '''\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def joint_ctf(tables, \n",
    "              sample_metadata: pd.DataFrame,\n",
    "              individual_id_column: str,\n",
    "              state_column: str,\n",
    "              #tensor_column: str, \n",
    "              n_components: int = DEFAULT_COMP,\n",
    "              ##done separately by user\n",
    "              ##also for rclr or other transformations\n",
    "              ##default can be same transformation\n",
    "              #min_sample_count: int = DEFAULT_MSC,\n",
    "              #min_feature_count: int = DEFAULT_MFC,\n",
    "              #min_feature_frequency: float = DEFAULT_MFF,\n",
    "              #transformation: Callable = matrix_rclr,\n",
    "              #pseudo_count: float = DEFAULT_TEMPTED_PC,\n",
    "              ##important to test dif modalities\n",
    "              replicate_handling: str = DEFAULT_TRH,\n",
    "              svd_centralized: bool = DEFAULT_TEMPTED_SVDC,\n",
    "              n_components_centralize: int = DEFAULT_TSCN,\n",
    "              smooth: float = DEFAULT_TEMPTED_SMTH,\n",
    "              resolution: int = DEFAULT_TEMPTED_RES,\n",
    "              max_iterations: int = DEFAULT_TEMPTED_MAXITER,\n",
    "              epsilon: float = DEFAULT_TEMPTED_EP) #-> (\n",
    "            #OrdinationResults,\n",
    "            #pd.DataFrame,\n",
    "            #DistanceMatrix,\n",
    "            #pd.DataFrame)):\n",
    "    '''\n",
    "    Joint decomposition of two or more tensors\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    tables: list of numpy.ndarray, required\n",
    "        List of feature tables (1-n) from different modalities\n",
    "        in biom format containing the samples over which\n",
    "        metrics should be computed.\n",
    "        Each modality should contain same number of samples\n",
    "        or individuals. Length of features might vary.\n",
    "    \n",
    "    sample_metadata: DataFrame, required\n",
    "        Sample metadata file in QIIME2 formatting. The file must\n",
    "        contain the columns for individual_id_column and\n",
    "        state_column and the rows matched to the table.\n",
    "\n",
    "    individual_id_column: str, required\n",
    "        Metadata column containing subject IDs to use for\n",
    "        pairing samples. WARNING: if replicates exist for an\n",
    "        individual ID at either state_1 to state_N, that\n",
    "        subject will be mean grouped by default.\n",
    "\n",
    "    state_column: str, required\n",
    "        Metadata column containing state (e.g.,Time,\n",
    "        BodySite) across which samples are paired. At least\n",
    "        one is required but up to four are allowed by other\n",
    "        state inputs.\n",
    "    \n",
    "    tensor_column: str, optional\n",
    "        Metadata column denoting which modality was \n",
    "        collected at a specific time point for each sample.\n",
    "        Note that for some time points, data from several   \n",
    "        modalities might be available \n",
    "\n",
    "    n_components: int, optional : Default is 3\n",
    "        The underlying rank of the data and number of\n",
    "        output dimentions.\n",
    "\n",
    "    min_sample_count: int, optional : Default is 0\n",
    "        Minimum sum cutoff of sample across all features.\n",
    "        The value can be at minimum zero and must be an\n",
    "        whole integer. It is suggested to be greater than\n",
    "        or equal to 500.\n",
    "\n",
    "    min_feature_count: int, optional : Default is 0\n",
    "        Minimum sum cutoff of features across all samples.\n",
    "        The value can be at minimum zero and must be\n",
    "        an whole integer.\n",
    "\n",
    "    min_feature_frequency: float, optional : Default is 0\n",
    "        Minimum percentage of samples a feature must appear\n",
    "        with a value greater than zero. This value can range\n",
    "        from 0 to 100 with decimal values allowed.\n",
    "\n",
    "    transformation: function, optional : Default is matrix_rclr\n",
    "        The transformation function to use on the data.\n",
    "\n",
    "    pseudo_count: float, optional : Default is 1\n",
    "        The pseudo count to add to all values before applying\n",
    "        the transformation.\n",
    "\n",
    "    replicate_handling: function, optional : Default is \"sum\"\n",
    "        Choose how replicate samples are handled. If replicates are\n",
    "        detected, \"error\" causes method to fail; \"drop\" will discard\n",
    "        all replicated samples; \"random\" chooses one representative at\n",
    "        random from among replicates.\n",
    "\n",
    "    svd_centralized: bool, optional : Default is True\n",
    "        Removes the mean structure of the temporal tensor.\n",
    "\n",
    "    n_components_centralize: int\n",
    "        Rank of approximation for average matrix in svd-centralize.\n",
    "\n",
    "    smooth: float, optional : Default is 1e-8\n",
    "        Smoothing parameter for RKHS norm. Larger means\n",
    "        smoother temporal loading functions.\n",
    "\n",
    "    resolution: int, optional : Default is 101\n",
    "        Number of time points to evaluate the value\n",
    "        of the temporal loading function.\n",
    "\n",
    "    max_iterations: int, optional : Default is 20\n",
    "        Maximum number of iteration in for rank-1 calculation.\n",
    "\n",
    "    epsilon: float, optional : Default is 0.0001\n",
    "        Convergence criteria for difference between iterations\n",
    "        for each rank-1 calculation.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    OrdinationResults\n",
    "        Compositional biplot of subjects as points and\n",
    "        features as arrows. Where the variation between\n",
    "        subject groupings is explained by the log-ratio\n",
    "        between opposing arrows.\n",
    "\n",
    "    DataFrame\n",
    "        Each components temporal loadings across the\n",
    "        input resolution included as a column called\n",
    "        'time_interval'.\n",
    "\n",
    "    DistanceMatrix\n",
    "        A subject-subject distance matrix generated\n",
    "        from the euclidean distance of the\n",
    "        subject ordinations and itself.\n",
    "\n",
    "    DataFrame\n",
    "        The loadings from the SVD centralize\n",
    "        function, used for projecting new data.\n",
    "        Warning: If SVD-centering is not used\n",
    "        then the function will add all ones as the\n",
    "        output to avoid variable outputs.\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    ValueError\n",
    "        if features don't match between tables\n",
    "        across the values of the dictionary\n",
    "    ValueError\n",
    "        if id_ not in mapping\n",
    "    ValueError\n",
    "        if any state_column not in mapping\n",
    "    ValueError\n",
    "        Table is not 2-dimensional\n",
    "    ValueError\n",
    "        Table contains negative values\n",
    "    ValueError\n",
    "        Table contains np.inf or -np.inf\n",
    "    ValueError\n",
    "        Table contains np.nan or missing.\n",
    "    Warning\n",
    "        If a conditional-sample pair\n",
    "        has multiple IDs associated\n",
    "        with it. In this case the\n",
    "        default method is to mean them.\n",
    "    ValueError\n",
    "        `ValueError: n_components must be at least 2`.\n",
    "    ValueError\n",
    "        `ValueError: Data-table contains\n",
    "         either np.inf or -np.inf`.\n",
    "    ValueError\n",
    "        `ValueError: The n_components must be less\n",
    "         than the minimum shape of the input tensor`.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    TODO\n",
    "    '''\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def joint_ctf_helper():\n",
    "    '''\n",
    "    Joint decomposition of two or more tensors\n",
    "    '''\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
