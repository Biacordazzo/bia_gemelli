{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import biom\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from typing import Callable\n",
    "from scipy.spatial import distance\n",
    "from skbio import (OrdinationResults,\n",
    "                   DistanceMatrix)\n",
    "from scipy.sparse.linalg import svds\n",
    "from gemelli.optspace import svd_sort\n",
    "from gemelli.ctf import ctf_table_processing\n",
    "from gemelli.preprocessing import (build_sparse,\n",
    "                                   matrix_rclr)\n",
    "from gemelli._defaults import (DEFAULT_COMP, DEFAULT_MSC,\n",
    "                               DEFAULT_MFC, DEFAULT_MFF,\n",
    "                               DEFAULT_TEMPTED_PC,\n",
    "                               DEFAULT_TEMPTED_EP,\n",
    "                               DEFAULT_TEMPTED_SMTH,\n",
    "                               DEFAULT_TEMPTED_RES,\n",
    "                               DEFAULT_TEMPTED_MAXITER,\n",
    "                               DEFAULT_TEMPTED_RH as DEFAULT_TRH,\n",
    "                               DEFAULT_TEMPTED_RHC as DEFAULT_RC,\n",
    "                               DEFAULT_TEMPTED_SVDC,\n",
    "                               DEFAULT_TEMPTED_SVDCN as DEFAULT_TSCN)\n",
    "\n",
    "from gemelli.tempted import (freg_rkhs, bernoulli_kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_time():\n",
    "    '''\n",
    "    Normalize time points to be in the same format and keep\n",
    "    only the defined interval (if defined)\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialization(n_individuals, tables_update):\n",
    "    \"\"\"\n",
    "    Initialize subject and feature loadings\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_individuals: int, required\n",
    "        Number of unique individuals/samples\n",
    "    tables_update: dictionary, required\n",
    "        Dictionary of tables constructed\n",
    "        (see build_sparse class).\n",
    "        keys = individual_ids\n",
    "        values = list of DataFrame, required\n",
    "            For each DataFrame (modality):\n",
    "            rows = features\n",
    "            columns = samples\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    a_hat: list of int\n",
    "        Updated subject loadings\n",
    "    b_hat: dictionary\n",
    "        Updated feature loadings\n",
    "        keys = modality\n",
    "        values = loadings\n",
    "        \n",
    "    Raises\n",
    "    ----------\n",
    "    TODO\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def udpate_tabular(tables_update, n_individuals,\n",
    "                   tipos, a_hat, phi_hats, b_hats):\n",
    "    '''\n",
    "    Update the tabular loadings (subjects and features) loadings\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    tables_update: dictionary, required\n",
    "        Dictionary of tables constructed\n",
    "        (see build_sparse class).\n",
    "        keys = individual_ids\n",
    "        values = list of DataFrame, required\n",
    "            For each DataFrame (modality):\n",
    "            rows = features\n",
    "            columns = samples\n",
    "    n_individuals: int, required\n",
    "        Number of unique individuals/samples\n",
    "    tipos: list of boolean, required\n",
    "        Time points to keep, based on the defined interval\n",
    "    a_hat: list of int, required\n",
    "        Subject loadings from the previous iteration\n",
    "    phi_hats: DataFrame, required\n",
    "        Temporal loadings from the previous iteration\n",
    "        rows = timepoints\n",
    "        columns = modality\n",
    "    b_hats: dictionary, required\n",
    "        Feature loadings from the previous iteration\n",
    "        keys = modality\n",
    "        values = loadings\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    a_new: list of int\n",
    "        Updated subject loadings\n",
    "    b_new: dictionary\n",
    "        Updated feature loadings\n",
    "        keys = modality\n",
    "        values = loadings\n",
    "        \n",
    "    Raises\n",
    "    ----------\n",
    "    TODO\n",
    "    '''\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_lambda(tables_update, tipos, \n",
    "                  a_hat, phi_hats, b_hats):\n",
    "    '''\n",
    "    Updates the singular values using the loadings\n",
    "    from the most recent iteration\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    tables_update: dictionary, required\n",
    "        Dictionary of tables constructed\n",
    "        (see build_sparse class).\n",
    "        keys = individual_ids\n",
    "        values = list of DataFrame, required\n",
    "            For each DataFrame (modality):\n",
    "            rows = features\n",
    "            columns = samples\n",
    "    tipos: list of boolean, required\n",
    "        Time points to keep, based on the defined interval\n",
    "    a_hat: list of int, required\n",
    "        Subject loadings from the previous iteration\n",
    "    phi_hats: DataFrame, required\n",
    "        Temporal loadings from the previous iteration\n",
    "        rows = timepoints\n",
    "        columns = modality\n",
    "    b_hats: dictionary, required\n",
    "        Feature loadings from the previous iteration\n",
    "        keys = modality\n",
    "        values = loadings\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    lambda_new: dictionary\n",
    "        Updated singular values\n",
    "        keys = modality\n",
    "        values = loadings\n",
    "        \n",
    "    Raises\n",
    "    ----------\n",
    "    TODO\n",
    "    '''\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def udpate_residuals():\n",
    "    '''\n",
    "    Update the tensor to be factorized by subtracting the \n",
    "    approximation the previous iteration\n",
    "    '''\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def joint_ctf(tables, ##NEW-ish\n",
    "              sample_metadata: pd.DataFrame,\n",
    "              individual_id_column: str,\n",
    "              state_column: str,\n",
    "              tensor_column: str, #NEW\n",
    "              n_components: int = DEFAULT_COMP,\n",
    "              min_sample_count: int = DEFAULT_MSC,\n",
    "              min_feature_count: int = DEFAULT_MFC,\n",
    "              min_feature_frequency: float = DEFAULT_MFF,\n",
    "              transformation: Callable = matrix_rclr,\n",
    "              pseudo_count: float = DEFAULT_TEMPTED_PC,\n",
    "              replicate_handling: str = DEFAULT_TRH,\n",
    "              svd_centralized: bool = DEFAULT_TEMPTED_SVDC,\n",
    "              n_components_centralize: int = DEFAULT_TSCN,\n",
    "              smooth: float = DEFAULT_TEMPTED_SMTH,\n",
    "              resolution: int = DEFAULT_TEMPTED_RES,\n",
    "              max_iterations: int = DEFAULT_TEMPTED_MAXITER,\n",
    "              epsilon: float = DEFAULT_TEMPTED_EP) #-> (\n",
    "            #OrdinationResults,\n",
    "            #pd.DataFrame,\n",
    "            #DistanceMatrix,\n",
    "            #pd.DataFrame)):\n",
    "    '''\n",
    "    Joint decomposition of two or more tensors\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    tables: list of numpy.ndarray, required\n",
    "        List of feature tables from different modalities\n",
    "        in biom format containing the samples over which\n",
    "        metrics should be computed.\n",
    "        Each modality should contain same number of samples\n",
    "        or individuals. Length of features might vary.\n",
    "    \n",
    "    sample_metadata: DataFrame, required\n",
    "        Sample metadata file in QIIME2 formatting. The file must\n",
    "        contain the columns for individual_id_column and\n",
    "        state_column and the rows matched to the table.\n",
    "    ##CHECK IF MULTIPLE METADATAS WILL BE NEEDED\n",
    "\n",
    "    individual_id_column: str, required\n",
    "        Metadata column containing subject IDs to use for\n",
    "        pairing samples. WARNING: if replicates exist for an\n",
    "        individual ID at either state_1 to state_N, that\n",
    "        subject will be mean grouped by default.\n",
    "\n",
    "    state_column: str, required\n",
    "        Metadata column containing state (e.g.,Time,\n",
    "        BodySite) across which samples are paired. At least\n",
    "        one is required but up to four are allowed by other\n",
    "        state inputs.\n",
    "    \n",
    "    ##NEW##\n",
    "    tensor_column: str, required\n",
    "        Metadata column denoting which modality was \n",
    "        collected at a specific time point for each sample.\n",
    "        Note that for some time points, data from several   \n",
    "        modalities might be available \n",
    "\n",
    "    n_components: int, optional : Default is 3\n",
    "        The underlying rank of the data and number of\n",
    "        output dimentions.\n",
    "\n",
    "    ##DO WE NEED TO ADJUST THE FILTERS BASED ON THE MODALITY?\n",
    "    ##IF SO, THIS COULD BE CHANGED TO LIST OF INT\n",
    "    min_sample_count: int, optional : Default is 0\n",
    "        Minimum sum cutoff of sample across all features.\n",
    "        The value can be at minimum zero and must be an\n",
    "        whole integer. It is suggested to be greater than\n",
    "        or equal to 500.\n",
    "\n",
    "    min_feature_count: int, optional : Default is 0\n",
    "        Minimum sum cutoff of features across all samples.\n",
    "        The value can be at minimum zero and must be\n",
    "        an whole integer.\n",
    "\n",
    "    min_feature_frequency: float, optional : Default is 0\n",
    "        Minimum percentage of samples a feature must appear\n",
    "        with a value greater than zero. This value can range\n",
    "        from 0 to 100 with decimal values allowed.\n",
    "\n",
    "    transformation: function, optional : Default is matrix_rclr\n",
    "        The transformation function to use on the data.\n",
    "\n",
    "    pseudo_count: float, optional : Default is 1\n",
    "        The pseudo count to add to all values before applying\n",
    "        the transformation.\n",
    "\n",
    "    replicate_handling: function, optional : Default is \"sum\"\n",
    "        Choose how replicate samples are handled. If replicates are\n",
    "        detected, \"error\" causes method to fail; \"drop\" will discard\n",
    "        all replicated samples; \"random\" chooses one representative at\n",
    "        random from among replicates.\n",
    "\n",
    "    svd_centralized: bool, optional : Default is True\n",
    "        Removes the mean structure of the temporal tensor.\n",
    "\n",
    "    n_components_centralize: int\n",
    "        Rank of approximation for average matrix in svd-centralize.\n",
    "\n",
    "    smooth: float, optional : Default is 1e-8\n",
    "        Smoothing parameter for RKHS norm. Larger means\n",
    "        smoother temporal loading functions.\n",
    "\n",
    "    resolution: int, optional : Default is 101\n",
    "        Number of time points to evaluate the value\n",
    "        of the temporal loading function.\n",
    "\n",
    "    max_iterations: int, optional : Default is 20\n",
    "        Maximum number of iteration in for rank-1 calculation.\n",
    "\n",
    "    epsilon: float, optional : Default is 0.0001\n",
    "        Convergence criteria for difference between iterations\n",
    "        for each rank-1 calculation.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    OrdinationResults\n",
    "        Compositional biplot of subjects as points and\n",
    "        features as arrows. Where the variation between\n",
    "        subject groupings is explained by the log-ratio\n",
    "        between opposing arrows.\n",
    "\n",
    "    DataFrame\n",
    "        Each components temporal loadings across the\n",
    "        input resolution included as a column called\n",
    "        'time_interval'.\n",
    "\n",
    "    DistanceMatrix\n",
    "        A subject-subject distance matrix generated\n",
    "        from the euclidean distance of the\n",
    "        subject ordinations and itself.\n",
    "\n",
    "    DataFrame\n",
    "        The loadings from the SVD centralize\n",
    "        function, used for projecting new data.\n",
    "        Warning: If SVD-centering is not used\n",
    "        then the function will add all ones as the\n",
    "        output to avoid variable outputs.\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    ValueError\n",
    "        if features don't match between tables\n",
    "        across the values of the dictionary\n",
    "    ValueError\n",
    "        if id_ not in mapping\n",
    "    ValueError\n",
    "        if any state_column not in mapping\n",
    "    ValueError\n",
    "        Table is not 2-dimensional\n",
    "    ValueError\n",
    "        Table contains negative values\n",
    "    ValueError\n",
    "        Table contains np.inf or -np.inf\n",
    "    ValueError\n",
    "        Table contains np.nan or missing.\n",
    "    Warning\n",
    "        If a conditional-sample pair\n",
    "        has multiple IDs associated\n",
    "        with it. In this case the\n",
    "        default method is to mean them.\n",
    "    ValueError\n",
    "        `ValueError: n_components must be at least 2`.\n",
    "    ValueError\n",
    "        `ValueError: Data-table contains\n",
    "         either np.inf or -np.inf`.\n",
    "    ValueError\n",
    "        `ValueError: The n_components must be less\n",
    "         than the minimum shape of the input tensor`.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    TODO\n",
    "    '''\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def joint_ctf_helper():\n",
    "    '''\n",
    "    Joint decomposition of two or more tensors\n",
    "    '''\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
