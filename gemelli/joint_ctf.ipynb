{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import biom\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from typing import Callable\n",
    "from scipy.spatial import distance\n",
    "from skbio import (OrdinationResults,\n",
    "                   DistanceMatrix)\n",
    "from scipy.sparse.linalg import svds\n",
    "\n",
    "from gemelli.optspace import svd_sort\n",
    "from gemelli.ctf import ctf_table_processing\n",
    "from gemelli.preprocessing import (build_sparse,\n",
    "                                   matrix_rclr)\n",
    "from gemelli._defaults import (DEFAULT_COMP, DEFAULT_MSC,\n",
    "                               DEFAULT_MFC, DEFAULT_MFF,\n",
    "                               DEFAULT_TEMPTED_PC,\n",
    "                               DEFAULT_TEMPTED_EP,\n",
    "                               DEFAULT_TEMPTED_SMTH,\n",
    "                               DEFAULT_TEMPTED_RES,\n",
    "                               DEFAULT_TEMPTED_MAXITER,\n",
    "                               DEFAULT_TEMPTED_RH as DEFAULT_TRH,\n",
    "                               DEFAULT_TEMPTED_RHC as DEFAULT_RC,\n",
    "                               DEFAULT_TEMPTED_SVDC,\n",
    "                               DEFAULT_TEMPTED_SVDCN as DEFAULT_TSCN)\n",
    "\n",
    "from gemelli.tempted import (freg_rkhs, bernoulli_kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_time(individual_id_tables,\n",
    "                individual_id_state_orders, \n",
    "                n_individuals, resolution, \n",
    "                timestamps_all, interval=None):\n",
    "    '''\n",
    "    Normalize time points to be in the same format and keep\n",
    "    only the defined interval (if defined)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    individual_id_tables: dictionary, required\n",
    "        Dictionary of tables constructed.\n",
    "        (see build_sparse class)\n",
    "        keys = individual_ids\n",
    "        values = DataFrame, required\n",
    "            rows = features\n",
    "            columns = samples\n",
    "\n",
    "    individual_id_state_orders : dict\n",
    "        Dictionary of time points for each individual\n",
    "\n",
    "    n_individuals: int, required\n",
    "        Number of unique individuals/samples\n",
    "\n",
    "    resolution: int, optional : Default is 101\n",
    "        Number of time points to evaluate the value\n",
    "        of the temporal loading function.\n",
    "\n",
    "    timestamps_all: list, required\n",
    "        Unique time points across all samples and\n",
    "        modalities\n",
    "\n",
    "    interval : tuple, optional\n",
    "        Start and end time points to keep\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    ti: list of numpy.ndarray\n",
    "        List of time points within defined interval \n",
    "        per subject\n",
    "\n",
    "    ind_vec: numpy.ndarray\n",
    "        Subject indexes for each time point\n",
    "\n",
    "    tm: numpy.ndarray\n",
    "        Concatenated normalized time points for all\n",
    "        subjects\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    TODO\n",
    "    '''\n",
    "\n",
    "    # make copy of tables to update\n",
    "    tables_update = copy.deepcopy(individual_id_tables)\n",
    "    orders_update = copy.deepcopy(individual_id_state_orders)\n",
    "    # set the interval if none is given\n",
    "    if interval is None:\n",
    "        interval = (timestamps_all[0], timestamps_all[-1])\n",
    "    # set time ranges [0, 1]\n",
    "    input_time_range = (timestamps_all[0], timestamps_all[-1])\n",
    "    # normalize time points\n",
    "    for individual_id in orders_update.keys():\n",
    "        orders_update[individual_id] = (orders_update[individual_id] - input_time_range[0]) \\\n",
    "                                        / (input_time_range[1] - input_time_range[0])\n",
    "    # ensure interval is in the same format\n",
    "    interval = tuple((interval - input_time_range[0]) \\\n",
    "                     / (input_time_range[1] - input_time_range[0]))\n",
    "    \n",
    "    # initialize variables to store time points (tps)\n",
    "    Lt = [] # all normalized tps\n",
    "    ind_vec = [] #individual indexes for each tp\n",
    "    ti = [[] for i in range(n_individuals)] # tps within interval per subject\n",
    "    \n",
    "    # populate variables above\n",
    "    for i, (id_, time_range_i) in enumerate(orders_update.items()):\n",
    "        # save all normalized time points\n",
    "        Lt.append(time_range_i)\n",
    "        ind_vec.extend([i] * len(Lt[-1]))\n",
    "        # define time points within interval\n",
    "        mask = (time_range_i >= interval[0]) & (time_range_i <= interval[1])\n",
    "        temp = time_range_i[mask]\n",
    "        temp = [(resolution-1)*(tp - interval[0])/(interval[1] - interval[0]) for tp in temp]\n",
    "        ti[i] = np.array(list(map(int, temp)))\n",
    "        # update tables and orders\n",
    "        tables_update[id_] = tables_update[id_].T[mask].T\n",
    "    \n",
    "    # convert variables to numpy arrays \n",
    "    ind_vec = np.array(ind_vec)\n",
    "    tm = np.concatenate(Lt)\n",
    "\n",
    "    return interval, tables_update, ti, ind_vec, tm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_tabular(individual_id_tables, \n",
    "                       n_individuals,\n",
    "                       n_components=3):\n",
    "                   \n",
    "    \"\"\"\n",
    "    Initialize subject and feature loadings\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    individual_id_tables: dictionary, required\n",
    "        Dictionary of tables constructed.\n",
    "        (see build_sparse class)\n",
    "        keys = individual_ids\n",
    "        values = DataFrame, required\n",
    "            rows = features\n",
    "            columns = samples\n",
    "\n",
    "    n_individuals: int, required\n",
    "        Number of unique individuals/samples\n",
    "\n",
    "    n_components: int, optional : Default is 3\n",
    "        The underlying rank of the data and number of\n",
    "        output dimentions.\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    b_hat: dictionary\n",
    "        Updated feature loadings\n",
    "        keys = modality\n",
    "        values = loadings\n",
    "        \n",
    "    Raises\n",
    "    ----------\n",
    "    TODO\n",
    "    \"\"\"\n",
    "\n",
    "    # initialize feature loadings\n",
    "    data_unfold = np.hstack([m.values for m in individual_id_tables.values()])\n",
    "    u, e, v = svds(data_unfold, k=n_components, which='LM')\n",
    "    u, e, v = svd_sort(u, np.diag(e), v)\n",
    "    b_hat = u[:, 0]\n",
    "\n",
    "    # initialize subject loadings\n",
    "    consistent_sign = np.sign(np.sum(b_hat))\n",
    "    a_hat = (np.ones(n_individuals) / np.sqrt(n_individuals)) * consistent_sign\n",
    "\n",
    "    return b_hat, a_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_lambda(individual_id_tables, ti, \n",
    "                  a_hat, phi_hat, b_hat):\n",
    "    '''\n",
    "    Updates the singular values using the loadings\n",
    "    from the most recent iteration\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    individual_id_tables: dictionary, required\n",
    "        Dictionary of tables constructed. Note that at this point\n",
    "        the tables have been subset to only include the time points\n",
    "        within the previously defined interval.\n",
    "        keys = individual_ids\n",
    "        values = DataFrame, required\n",
    "            rows = features\n",
    "            columns = samples\n",
    "    ti: list of int, required\n",
    "        Time points within predefined interval for\n",
    "        each individual \n",
    "    a_hat: np.narray, required\n",
    "        Subject loadings from the previous iteration\n",
    "    phi_hats: np.narray, required\n",
    "        Temporal loadings from the previous iteration\n",
    "    b_hat: np.narray, required\n",
    "        Feature loadings from the previous iteration\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    lambda_new: dictionary\n",
    "        Updated singular values\n",
    "        keys = modality\n",
    "        values = loadings\n",
    "        \n",
    "    Raises\n",
    "    ----------\n",
    "    TODO\n",
    "    '''\n",
    "    \n",
    "    nums = []\n",
    "    denoms = []\n",
    "\n",
    "    for i, m in enumerate(individual_id_tables.values()):\n",
    "        \n",
    "        phi_ = phi_hat[ti[i]]\n",
    "        num = a_hat[i]*(b_hat.dot(m.values).dot(phi_))\n",
    "        nums.append(num)\n",
    "        denom = (a_hat[i]*phi_) ** 2\n",
    "        denom = np.sum(denom)\n",
    "        denoms.append(denom)\n",
    "    \n",
    "    lambda_new = np.sum(nums) / np.sum(denoms)\n",
    "\n",
    "    return lambda_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_a_mod(individual_id_tables, \n",
    "                 n_individuals, n_features,\n",
    "                 b_mod, phi_mod, \n",
    "                 lambda_mod, ti):\n",
    "    '''\n",
    "    Update the tabular loadings (subjects and features) loadings\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    individual_id_tables: dictionary, required\n",
    "        Dictionary of tables constructed. Note that at this point\n",
    "        the tables have been subset to only include the time points\n",
    "        within the previously defined interval.\n",
    "        keys = individual_ids\n",
    "        values = DataFrame, required\n",
    "            rows = features\n",
    "            columns = samples\n",
    "\n",
    "    n_individuals: int, required\n",
    "        Number of unique individuals/samples in modality\n",
    "\n",
    "    n_features: int, required\n",
    "        Number of unique features in modality\n",
    "\n",
    "    b_mod: np.narray, required\n",
    "        Feature loadings from a specific modality\n",
    "\n",
    "    phi_mod: np.narray, required\n",
    "        Temporal loadings from a specific modality\n",
    "\n",
    "    lambda_mod: float, required\n",
    "        Singular value from a specific modality\n",
    "\n",
    "    ti: list of int, required\n",
    "        Time points within predefined interval for\n",
    "        each individual\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    a_new: np.narray\n",
    "        Updated subject loadings\n",
    "    b_new: np.narray\n",
    "        Updated feature loadings\n",
    "        \n",
    "    Raises\n",
    "    ----------\n",
    "    TODO\n",
    "    '''\n",
    "\n",
    "    #initialize intermediate outputs\n",
    "    a_num = {}\n",
    "    a_denom = {}\n",
    "    b_num = np.zeros((n_features, n_individuals))\n",
    "    common_denom = {}\n",
    "\n",
    "    for i, (individual_id, m) in enumerate(individual_id_tables.items()):\n",
    "\n",
    "        #keep only relevant timepoints (within interval)\n",
    "        phi_ = phi_mod[ti[i]]\n",
    "        #save item needed for both a_hat and b_hat\n",
    "        common_denom[individual_id] = np.sum(phi_ ** 2)\n",
    "        #save item needed later for b_hat\n",
    "        b_num[:, i] = (m.values).dot(phi_)  #vector per individual\n",
    "        #a_hat specific operations\n",
    "        a_num_mod = lambda_mod*b_mod.dot(m.values).dot(phi_)\n",
    "        a_num[individual_id] = a_num_mod\n",
    "        a_denom[individual_id] = (lambda_mod ** 2)*common_denom[individual_id]\n",
    "\n",
    "    return a_num, a_denom, b_num, common_denom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatting_iter(individual_id_tables, \n",
    "                    individual_id_state_orders,\n",
    "                    mod_id_ind, timestamps_all, \n",
    "                    interval, resolution):\n",
    "    '''\n",
    "    Format the input data for downstream tasks and \n",
    "    calculate tne kernel matrix.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    individual_id_tables: dictionary, required\n",
    "        Dictionary of 1 to n tables constructed,\n",
    "        (see build_sparse class), where n is the \n",
    "        number of modalities.\n",
    "        keys = individual_ids\n",
    "        values = list of DataFrame, required\n",
    "            For each DataFrame (modality):\n",
    "            rows = features\n",
    "            columns = samples\n",
    "\n",
    "    individual_id_state_orders: dictionary, required\n",
    "        Dictionary of 1 to n lists of time points (one \n",
    "        per modality) for each sample.\n",
    "        keys = individual_ids\n",
    "        values = list of numpy.ndarray\n",
    "            Each numpy.ndarray contains the time points\n",
    "            of the corresponding modality\n",
    "            Note: array of dtype=object to allow for\n",
    "            different number of time points per modality\n",
    "\n",
    "    mod_id_ind: dictionary, required\n",
    "        Dictionary of individual IDs for each modality\n",
    "        keys = modality\n",
    "        values = list of tuples\n",
    "            Each tuple contains the individual id and \n",
    "            the dataframe index in individual_id_tables\n",
    "\n",
    "    timestamps_all: list, required\n",
    "        Unique time points across all samples and\n",
    "        modalities\n",
    "\n",
    "    interval : tuple, optional\n",
    "        Start and end time points to keep\n",
    "\n",
    "    resolution: int, optional : Default is 101\n",
    "        Number of time points to evaluate the value\n",
    "        of the temporal loading function.\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    table_mods: dictionary\n",
    "        Updated tables for each modality. Times are\n",
    "        normalized and only points within the interval\n",
    "        are kept.\n",
    "        keys = modality\n",
    "        values = DataFrame\n",
    "            rows = features\n",
    "            columns = samples\n",
    "\n",
    "    times: dictionary\n",
    "        Updated time points for each modality\n",
    "        keys = modality\n",
    "        values = list of numpy.ndarray\n",
    "            list[0] = time points within interval\n",
    "            list[1] = individual indexes\n",
    "\n",
    "    Kmats: dictionary\n",
    "        Kernel matrix for each modality\n",
    "        keys = modality\n",
    "        values = numpy.ndarray\n",
    "            rows, columns = time points\n",
    "    \n",
    "    Kmat_outputs: dictionary\n",
    "        Bernoulli kernel matrix for each modality\n",
    "        keys = modality\n",
    "        values = numpy.ndarray\n",
    "            rows = resolution\n",
    "            columns = time points\n",
    "\n",
    "    Raises\n",
    "    ----------\n",
    "    TODO\n",
    "    '''\n",
    "\n",
    "    #initialize dictionary to store outputs per modality\n",
    "    #To-do: save in self\n",
    "    table_mods = {}\n",
    "    times = {}\n",
    "    Kmats = {}\n",
    "    Kmat_outputs = {}\n",
    "    \n",
    "    #iterate through each modality\n",
    "    for modality in mod_id_ind.keys():        \n",
    "\n",
    "        #get the individual IDs\n",
    "        ind_tuple_lst = mod_id_ind[modality]\n",
    "        #keep modality-specific time points\n",
    "        orders_mod = {ind[0]: individual_id_state_orders[ind[0]][ind[1]] \n",
    "                      for ind in ind_tuple_lst}\n",
    "        #keep modality-specific tables\n",
    "        table_mod = {ind[0]: individual_id_tables[ind[0]][ind[1]] \n",
    "                     for ind in ind_tuple_lst}\n",
    "        n_individuals = len(table_mod)\n",
    "        #format time points and keep points in the interval\n",
    "        (norm_interval, table_mod, \n",
    "         ti, ind_vec, tm) = format_time(table_mod, orders_mod,\n",
    "                                        n_individuals, resolution,\n",
    "                                        timestamps_all, interval)\n",
    "        #save key outputs\n",
    "        table_mods[modality] = table_mod\n",
    "        times[modality] = [ti, ind_vec]\n",
    "        #construct the kernel matrix\n",
    "        Kmats[modality] = bernoulli_kernel(tm, tm)\n",
    "        Kmat_outputs[modality] = bernoulli_kernel(np.linspace(norm_interval[0],\n",
    "                                                              norm_interval[1],\n",
    "                                                              num=resolution),\n",
    "                                                 tm)\n",
    "    \n",
    "    return table_mods, times, Kmats, Kmat_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decomposition_iter(table_mods, times, \n",
    "                       individual_id_lst, \n",
    "                       Kmats, Kmat_outputs,\n",
    "                       maxiter=20, epsilon=1e-4,\n",
    "                       smooth=1e-6, n_components=3):\n",
    "    '''\n",
    "    Iterate over the available modalities\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    table_mods: dictionary, required\n",
    "        Updated tables for each modality. Times are\n",
    "        normalized and only points within the interval\n",
    "        are kept.\n",
    "        keys = modality\n",
    "        values = DataFrame\n",
    "            rows = features\n",
    "            columns = samples\n",
    "\n",
    "    individual_id_lst: list, required\n",
    "        List of unique individual IDs\n",
    "\n",
    "    times: dictionary, required\n",
    "        Updated time points for each modality\n",
    "        keys = modality\n",
    "        values = list of numpy.ndarray\n",
    "            list[0] = time points within interval\n",
    "            list[1] = individual indexes\n",
    "\n",
    "    Kmats: dictionary, required\n",
    "        Kernel matrix for each modality\n",
    "        keys = modality\n",
    "        values = numpy.ndarray\n",
    "            rows, columns = time points\n",
    "    \n",
    "    Kmat_outputs: dictionary, required\n",
    "        Bernoulli kernel matrix for each modality\n",
    "        keys = modality\n",
    "        values = numpy.ndarray\n",
    "            rows = resolution\n",
    "            columns = time points\n",
    "\n",
    "    maxiter: int, optional : Default is 20\n",
    "        Maximum number of iteration in for rank-1 calculation\n",
    "\n",
    "    epsilon: float, optional : Default is 0.0001\n",
    "        Convergence criteria for difference between iterations\n",
    "        for each rank-1 calculation.\n",
    "\n",
    "    smooth: float, optional : Default is 1e-6\n",
    "        Smoothing parameter for the kernel matrix\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    Rank-1 loadings\n",
    "    a_hat: np.narray\n",
    "        Subject loadings, shared across modalities\n",
    "\n",
    "    b_hats: dictionary\n",
    "        Feature loadings\n",
    "        keys = modality\n",
    "        values = loadings\n",
    "\n",
    "    phi_hats: dictionary\n",
    "        Temporal loadings\n",
    "        keys = modality\n",
    "        values = loadings\n",
    "\n",
    "    lambdas: dictionary\n",
    "        Singular values\n",
    "        keys = modality\n",
    "        values = loadings\n",
    "        \n",
    "    Raises\n",
    "    ----------\n",
    "    TODO\n",
    "    '''\n",
    "\n",
    "    a_hats = {}\n",
    "    b_hats = {}\n",
    "    phi_hats = {}\n",
    "    lambdas = {}\n",
    "    common_denom = {}\n",
    "    b_num = {}\n",
    "    \n",
    "    #iterate until convergence\n",
    "    t = 0\n",
    "    dif = 1\n",
    "    while t <= maxiter and dif > epsilon:            \n",
    "        \n",
    "        #variables to save intermediate outputs\n",
    "        a_num = {}\n",
    "        a_denom = {}\n",
    "        b_hat_difs = {}\n",
    "        for modality in table_mod.keys():\n",
    "            \n",
    "            #get key modality-specific variables\n",
    "            table_mod = table_mods[modality]\n",
    "            ti, ind_vec = times[modality]\n",
    "            Kmat = Kmats[modality]\n",
    "            Kmat_output = Kmat_outputs[modality]\n",
    "            n_individuals = len(table_mod)\n",
    "            n_features = table_mod[0].shape[0]\n",
    "            \n",
    "            if t == 0:\n",
    "                #initialize feature and subject loadings\n",
    "                data_unfold = np.hstack([m.values for m in table_mod.values()])\n",
    "                b_hat, a_hat = initialize_tabular(data_unfold, \n",
    "                                                  n_individuals=n_individuals,\n",
    "                                                  n_components=n_components)\n",
    "                b_hats[modality] = b_hat\n",
    "                a_hats[modality] = a_hat\n",
    "            if t > 0:\n",
    "                #update feature loadings\n",
    "                b_temp = b_num[modality]\n",
    "                b_new = b_temp.dot(a_hat) / (common_denom[modality].dot(a_hat ** 2))\n",
    "                b_hat = b_new / np.sqrt(np.sum(b_new ** 2))\n",
    "                b_hat_difs[modality] = np.sum((b_hats[modality] - b_hat) ** 2)\n",
    "                b_hats[modality] = b_hat\n",
    "            \n",
    "            #calculate state loadings\n",
    "            Ly = [a_hat[i] * b_hat.dot(m) for i, m in enumerate(table_mod.values())]\n",
    "            phi_hat = freg_rkhs(Ly, a_hat, ind_vec, Kmat, Kmat_output, smooth=smooth)\n",
    "            phi_hat = (phi_hat / np.sqrt(np.sum(phi_hat ** 2)))\n",
    "            phi_hats[modality] = phi_hat\n",
    "            #calculate lambda\n",
    "            lambda_mod = update_lambda(table_mod, ti, a_hat, phi_hat, b_hat)\n",
    "            lambdas[modality] = lambda_mod\n",
    "            #begin updating subject and feature loadings\n",
    "            (a_mod_num, a_mod_denom, \n",
    "             b_mod_num, common_mod_denom) = update_a_mod(table_mod, n_individuals, n_features,\n",
    "                                                         b_hat, phi_hat, lambda_mod, ti)\n",
    "            #save intermediate b-hat variables\n",
    "            b_num[modality] = b_mod_num\n",
    "            common_denom[modality] = common_mod_denom\n",
    "            #add subject loading variables\n",
    "            a_num = {**a_num, **{key: a_mod_num[key] + a_num.get(key, 0) \n",
    "                                 for key in a_mod_num}}\n",
    "            a_denom = {**a_denom, **{key: a_mod_denom[key] + a_denom.get(key, 0) \n",
    "                                     for key in a_mod_denom}}\n",
    "        #update subject loadings\n",
    "        a_tilde = [a_num[id] / a_denom[id] for id in individual_id_lst]\n",
    "        a_new = a_tilde / np.sqrt(np.sum(a_tilde ** 2))\n",
    "        a_hat_dif = np.sum((a_hat - a_new) ** 2)\n",
    "        a_hat = a_new\n",
    "        #check for convergence\n",
    "        dif = max([a_hat_dif]+list(b_hat_difs.values())) #or take mean of b_hat_difs?    \n",
    "        t += 1\n",
    "\n",
    "    return a_hat, b_hats, phi_hats, lambdas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def udpate_residuals():\n",
    "    '''\n",
    "    Update the tensor to be factorized by subtracting the \n",
    "    approximation the previous iteration\n",
    "    '''\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class concat_tensors():\n",
    "\n",
    "    '''\n",
    "    Concatenate the tensors from each modality into a\n",
    "    single tensor class\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    tensors: dictionary, required\n",
    "        Dictionary of tensors constructed.\n",
    "        keys = modality\n",
    "        values = tensor, required\n",
    "            rows = features\n",
    "            columns = samples\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    self: object\n",
    "        Returns the instance itself\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def concat(self, tensors):\n",
    "        '''\n",
    "        Concatenate tensors from each modality into a\n",
    "        single tensor. Note: tensors should have been\n",
    "        preprocessed by this point.\n",
    "        '''\n",
    "\n",
    "        individual_id_tables = {}\n",
    "        individual_id_state_orders = {}\n",
    "        mod_id_ind = {}\n",
    "        \n",
    "        for mod, tensor in tensors.items():\n",
    "            \n",
    "            #concatenate tables\n",
    "            for ind_id, table in tensor.individual_id_tables_centralized.items():\n",
    "                individual_id_tables[ind_id] = individual_id_tables.get(ind_id, []) + [table]\n",
    "                mod_id_ind[mod] = mod_id_ind.get(mod, []) + [(ind_id, len(individual_id_tables[ind_id])-1)]\n",
    "            #concatenate state orders\n",
    "            for ind_id, order in tensor.individual_id_state_orders.items():\n",
    "                individual_id_state_orders[ind_id] = individual_id_state_orders.get(ind_id, []) + [order]\n",
    "\n",
    "        ##TODO make sure individuals are ordered the same way in all dictionaries?\n",
    "        \n",
    "        #store all to self\n",
    "        self.individual_id_tables = individual_id_tables\n",
    "        self.individual_id_state_orders = individual_id_state_orders\n",
    "        self.mod_id_ind = mod_id_ind\n",
    "\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def joint_ctf(tables, \n",
    "              sample_metadatas,\n",
    "              modality_ids,\n",
    "              individual_id_column: str,\n",
    "              state_column: str,\n",
    "              n_components: int = DEFAULT_COMP,\n",
    "              ##could be done separately by user\n",
    "              ##also for rclr or other transformations\n",
    "              ##default can be same transformation\n",
    "              min_sample_count: int = DEFAULT_MSC,\n",
    "              min_feature_count: int = DEFAULT_MFC,\n",
    "              min_feature_frequency: float = DEFAULT_MFF,\n",
    "              transformation: Callable = matrix_rclr,\n",
    "              pseudo_count: float = DEFAULT_TEMPTED_PC,\n",
    "              ##important to test dif modalities\n",
    "              replicate_handling: str = DEFAULT_TRH,\n",
    "              svd_centralized: bool = DEFAULT_TEMPTED_SVDC,\n",
    "              n_components_centralize: int = DEFAULT_TSCN,\n",
    "              smooth: float = DEFAULT_TEMPTED_SMTH,\n",
    "              resolution: int = DEFAULT_TEMPTED_RES,\n",
    "              max_iterations: int = DEFAULT_TEMPTED_MAXITER,\n",
    "              epsilon: float = DEFAULT_TEMPTED_EP) #-> (\n",
    "            #OrdinationResults,\n",
    "            #pd.DataFrame,\n",
    "            #DistanceMatrix,\n",
    "            #pd.DataFrame)):\n",
    "    '''\n",
    "    Joint decomposition of two or more tensors\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    tables: list of numpy.ndarray, required\n",
    "        List of feature tables (1-n) from different modalities\n",
    "        in biom format containing the samples over which\n",
    "        metrics should be computed.\n",
    "        Each modality should contain same number of samples\n",
    "        or individuals. Length of features might vary.\n",
    "    \n",
    "    sample_metadatas: list of DataFrame, required\n",
    "        Sample metadata files in QIIME2 formatting for each \n",
    "        modality. The file must contain the columns for \n",
    "        individual_id_column and state_column and the rows\n",
    "        matched to the table.\n",
    "\n",
    "    individual_id_column: str, required\n",
    "        Metadata column containing subject IDs to use for\n",
    "        pairing samples. WARNING: if replicates exist for an\n",
    "        individual ID at either state_1 to state_N, that\n",
    "        subject will be mean grouped by default.\n",
    "\n",
    "    state_column: str, required\n",
    "        Metadata column containing state (e.g.,Time,\n",
    "        BodySite) across which samples are paired. At least\n",
    "        one is required but up to four are allowed by other\n",
    "        state inputs.\n",
    "\n",
    "    n_components: int, optional : Default is 3\n",
    "        The underlying rank of the data and number of\n",
    "        output dimentions.\n",
    "\n",
    "    min_sample_count: int, optional : Default is 0\n",
    "        Minimum sum cutoff of sample across all features.\n",
    "        The value can be at minimum zero and must be an\n",
    "        whole integer. It is suggested to be greater than\n",
    "        or equal to 500.\n",
    "\n",
    "    min_feature_count: int, optional : Default is 0\n",
    "        Minimum sum cutoff of features across all samples.\n",
    "        The value can be at minimum zero and must be\n",
    "        an whole integer.\n",
    "\n",
    "    min_feature_frequency: float, optional : Default is 0\n",
    "        Minimum percentage of samples a feature must appear\n",
    "        with a value greater than zero. This value can range\n",
    "        from 0 to 100 with decimal values allowed.\n",
    "\n",
    "    transformation: function, optional : Default is matrix_rclr\n",
    "        The transformation function to use on the data.\n",
    "\n",
    "    pseudo_count: float, optional : Default is 1\n",
    "        The pseudo count to add to all values before applying\n",
    "        the transformation.\n",
    "\n",
    "    replicate_handling: function, optional : Default is \"sum\"\n",
    "        Choose how replicate samples are handled. If replicates are\n",
    "        detected, \"error\" causes method to fail; \"drop\" will discard\n",
    "        all replicated samples; \"random\" chooses one representative at\n",
    "        random from among replicates.\n",
    "\n",
    "    svd_centralized: bool, optional : Default is True\n",
    "        Removes the mean structure of the temporal tensor.\n",
    "\n",
    "    n_components_centralize: int\n",
    "        Rank of approximation for average matrix in svd-centralize.\n",
    "\n",
    "    smooth: float, optional : Default is 1e-8\n",
    "        Smoothing parameter for RKHS norm. Larger means\n",
    "        smoother temporal loading functions.\n",
    "\n",
    "    resolution: int, optional : Default is 101\n",
    "        Number of time points to evaluate the value\n",
    "        of the temporal loading function.\n",
    "\n",
    "    max_iterations: int, optional : Default is 20\n",
    "        Maximum number of iteration in for rank-1 calculation.\n",
    "\n",
    "    epsilon: float, optional : Default is 0.0001\n",
    "        Convergence criteria for difference between iterations\n",
    "        for each rank-1 calculation.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    OrdinationResults\n",
    "        Compositional biplot of subjects as points and\n",
    "        features as arrows. Where the variation between\n",
    "        subject groupings is explained by the log-ratio\n",
    "        between opposing arrows.\n",
    "\n",
    "    DataFrame\n",
    "        Each components temporal loadings across the\n",
    "        input resolution included as a column called\n",
    "        'time_interval'.\n",
    "\n",
    "    DistanceMatrix\n",
    "        A subject-subject distance matrix generated\n",
    "        from the euclidean distance of the\n",
    "        subject ordinations and itself.\n",
    "\n",
    "    DataFrame\n",
    "        The loadings from the SVD centralize\n",
    "        function, used for projecting new data.\n",
    "        Warning: If SVD-centering is not used\n",
    "        then the function will add all ones as the\n",
    "        output to avoid variable outputs.\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    ValueError\n",
    "        if features don't match between tables\n",
    "        across the values of the dictionary\n",
    "    ValueError\n",
    "        if id_ not in mapping\n",
    "    ValueError\n",
    "        if any state_column not in mapping\n",
    "    ValueError\n",
    "        Table is not 2-dimensional\n",
    "    ValueError\n",
    "        Table contains negative values\n",
    "    ValueError\n",
    "        Table contains np.inf or -np.inf\n",
    "    ValueError\n",
    "        Table contains np.nan or missing.\n",
    "    Warning\n",
    "        If a conditional-sample pair\n",
    "        has multiple IDs associated\n",
    "        with it. In this case the\n",
    "        default method is to mean them.\n",
    "    ValueError\n",
    "        `ValueError: n_components must be at least 2`.\n",
    "    ValueError\n",
    "        `ValueError: Data-table contains\n",
    "         either np.inf or -np.inf`.\n",
    "    ValueError\n",
    "        `ValueError: The n_components must be less\n",
    "         than the minimum shape of the input tensor`.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    TODO\n",
    "    '''\n",
    "    \n",
    "    #note: we assume each modality has a dif table and associated\n",
    "    #metadata. We also assume filtering conditions are the same\n",
    "    tensors = {}\n",
    "    for table, sample_metadata, mod_ids in zip(tables, \n",
    "                                               sample_metadatas,\n",
    "                                               modality_ids):\n",
    "    \n",
    "        # check the table for validity and then filter\n",
    "        process_results = ctf_table_processing(table,\n",
    "                                               sample_metadata,\n",
    "                                               individual_id_column,\n",
    "                                               [state_column],\n",
    "                                               min_sample_count,\n",
    "                                               min_feature_count,\n",
    "                                               min_feature_frequency,\n",
    "                                               None)\n",
    "        table = process_results[0]\n",
    "        sample_metadata = process_results[1]\n",
    "        # build the sparse tensor format\n",
    "        tensor = build_sparse()\n",
    "        tensor.construct(table,\n",
    "                        sample_metadata,\n",
    "                        individual_id_column,\n",
    "                        state_column,\n",
    "                        transformation=transformation,\n",
    "                        pseudo_count=pseudo_count,\n",
    "                        branch_lengths=None,\n",
    "                        replicate_handling=replicate_handling,\n",
    "                        svd_centralized=svd_centralized,\n",
    "                        n_components_centralize=n_components_centralize)\n",
    "        tensors[mod_ids] = tensor\n",
    "    #save all tensors to a class\n",
    "    n_tensors = concat_tensors().concat(tensors)\n",
    "    \n",
    "    # run joint-CTF\n",
    "    joint_ctf_res = joint_ctf_helper(n_tensors.individual_id_tables_centralized,\n",
    "                                     n_tensors.individual_id_state_orders,\n",
    "                                     n_tensors.feature_order,\n",
    "                                     n_components=n_components,\n",
    "                                     smooth=smooth,\n",
    "                                     resolution=resolution,\n",
    "                                     maxiter=max_iterations,\n",
    "                                     epsilon=epsilon)\n",
    "    (individual_loadings,\n",
    "     feature_loadings,\n",
    "     state_loadings,\n",
    "     time_return,\n",
    "     eigenvalues,\n",
    "     prop_explained) = joint_ctf_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def joint_ctf_helper(individual_id_tables,\n",
    "                     individual_id_state_orders,\n",
    "                     mod_id_ind, interval,\n",
    "                     resolution, maxiter,\n",
    "                     epsilon, smooth, \n",
    "                     n_components):\n",
    "    '''\n",
    "    Joint decomposition of two or more tensors\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    individual_id_tables: dictionary, required\n",
    "        Dictionary of 1 to n tables constructed,\n",
    "        (see build_sparse class), where n is the \n",
    "        number of modalities.\n",
    "        keys = individual_ids\n",
    "        values = list of DataFrame, required\n",
    "            For each DataFrame (modality):\n",
    "            rows = features\n",
    "            columns = samples\n",
    "\n",
    "    individual_id_state_orders: dictionary, required\n",
    "        Dictionary of 1 to n lists of time points (one \n",
    "        per modality) for each sample.\n",
    "        keys = individual_ids\n",
    "        values = list of numpy.ndarray\n",
    "            Each numpy.ndarray contains the time points\n",
    "            of the corresponding modality\n",
    "            Note: array of dtype=object to allow for\n",
    "            different number of time points per modality\n",
    "\n",
    "    mod_id_ind: dictionary, required\n",
    "        Dictionary of individual IDs for each modality\n",
    "        keys = modality\n",
    "        values = list of tuples\n",
    "            Each tuple contains the individual id and \n",
    "            the dataframe index in individual_id_tables\n",
    "\n",
    "    interval : tuple, optional\n",
    "        Start and end time points to keep\n",
    "\n",
    "    resolution: int, optional : Default is 101\n",
    "        Number of time points for the temporal \n",
    "        loading function.\n",
    "\n",
    "    maxiter: int, optional : Default is 20\n",
    "        Maximum number of iteration in for rank-1 calculation.\n",
    "\n",
    "    epsilon: float, optional : Default is 0.0001\n",
    "        Convergence criteria for difference between iterations\n",
    "        for each rank-1 calculation.\n",
    "\n",
    "    smooth: float, optional : Default is 1e-8\n",
    "        Smoothing parameter for RKHS norm. Larger means\n",
    "        smoother temporal loading functions.\n",
    "\n",
    "    n_components: int, optional : Default is 3\n",
    "        The underlying rank of the data and number of\n",
    "        output dimentions.\n",
    "    '''\n",
    "    \n",
    "    #make copy of tables to update\n",
    "    tables_update = copy.deepcopy(individual_id_tables)\n",
    "    orders_update = copy.deepcopy(individual_id_state_orders)\n",
    "    #get all individual IDs\n",
    "    individual_id_lst = list(orders_update.keys())\n",
    "    n_individuals_all = len(individual_id_lst)\n",
    "    #get all time points across all modalities\n",
    "    timestamps_all = np.concatenate(list(orders_update.values()))\n",
    "    timestamps_all = np.concatenate(timestamps_all)\n",
    "    timestamps_all = np.unique(timestamps_all)\n",
    "    #format time points and keep points in defined interval\n",
    "    (table_mods, times,\n",
    "    Kmats, Kmat_outputs) = formatting_iter(tables_update, \n",
    "                                           orders_update,\n",
    "                                           mod_id_ind, \n",
    "                                           timestamps_all,\n",
    "                                           interval, resolution)\n",
    "    #init dataframes to fill\n",
    "    #key: component number, value: dictionary of modality-specific loadings\n",
    "    n_component_col_names = ['component_' + str(i+1)\n",
    "                             for i in range(n_components)]\n",
    "    individual_loadings = pd.DataFrame(np.zeros((n_individuals_all, n_components)),\n",
    "                                       index=tables_update.keys(),\n",
    "                                       columns=n_component_col_names)\n",
    "    feature_loadings = {}\n",
    "    state_loadings = {}\n",
    "    lambda_coeff = {} \n",
    "\n",
    "    #perform decomposition\n",
    "    for r in n_components:\n",
    "        comp_name = 'component_' + str(r+1)\n",
    "        (a_hat, b_hats, \n",
    "         phi_hats, lambdas) = decomposition_iter(table_mods, times,\n",
    "                                                 individual_id_lst,\n",
    "                                                 Kmats, Kmat_outputs,\n",
    "                                                 maxiter, epsilon, \n",
    "                                                 smooth, n_components)\n",
    "        #save rank-1 components\n",
    "        individual_loadings.iloc[:, r] = a_hat\n",
    "        feature_loadings[comp_name] = b_hats\n",
    "        state_loadings[comp_name] = phi_hats\n",
    "        lambda_coeff[comp_name] = lambdas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sanity checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dummy orders_update\n",
    "individual_id_state_orders = {'ind1': np.array([[0, 0.5, 1, 2],[0, 0.5, 1],[0, 0.5, 1]], dtype=object),\n",
    "                              'ind2': np.array([[0, 1, 3],[0, 0.5, 1, 3]], dtype=object)}\n",
    "\n",
    "individual_id_state_orders2 = {'ind1': np.array([0, 0.5, 1, 2]),\n",
    "                              'ind2': np.array([0, 1, 3])}\n",
    "\n",
    "print(individual_id_state_orders)\n",
    "print(individual_id_state_orders2)\n",
    "\n",
    "#create random tables\n",
    "table1 = np.random.randint(0,100,size=(10, 4))\n",
    "table2 = np.random.randint(0,100,size=(12, 4))\n",
    "\n",
    "table3 = np.random.randint(0,100,size=(10, 3))\n",
    "table4 = np.random.randint(0,100,size=(12, 4))\n",
    "\n",
    "#create dictionary of tables\n",
    "#individual_id_tables = {'ind1': [table1, table2], 'ind2': [table3, table4]}\n",
    "#individual_id_mod = {'ind1': table1, 'ind2': table3}\n",
    "\n",
    "tensor1_tables = {'ind1': table1, 'ind2': table2, 'ind4': table1}\n",
    "tensor2_tables = {'ind1': table3, 'ind2': table4, 'ind3': table3}\n",
    "\n",
    "tensor1_state_orders = {'ind1': [0, 0.5, 1, 2], 'ind2': [0, 0.5, 1, 3], 'ind4': [0, 0.5, 1, 3]}\n",
    "tensor2_state_orders = {'ind1': [0, 0.5, 1], 'ind2': [0, 0.5, 1, 3], 'ind3': [0, 0.5, 2]}\n",
    "\n",
    "individual_id_orders = {}\n",
    "individual_id_tables = {}\n",
    "\n",
    "#concat lst from both tensors by individual\n",
    "for tensor in [tensor1_state_orders, tensor2_state_orders]:\n",
    "\n",
    "    for key, value in tensor.items():\n",
    "        individual_id_orders[key] = individual_id_orders.get(key, []) + [value]\n",
    "\n",
    "for tensor in [tensor1_tables, tensor2_tables]:\n",
    "\n",
    "    for key, value in tensor.items():\n",
    "        individual_id_tables[key] = individual_id_tables.get(key, []) + [value]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
