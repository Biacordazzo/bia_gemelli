{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import biom\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from typing import Callable\n",
    "from scipy.spatial import distance\n",
    "from skbio import (OrdinationResults,\n",
    "                   DistanceMatrix)\n",
    "from scipy.sparse.linalg import svds\n",
    "\n",
    "from gemelli.optspace import svd_sort\n",
    "from gemelli.ctf import ctf_table_processing\n",
    "from gemelli.preprocessing import (build_sparse,\n",
    "                                   matrix_rclr)\n",
    "from gemelli._defaults import (DEFAULT_COMP, DEFAULT_MSC,\n",
    "                               DEFAULT_MFC, DEFAULT_MFF,\n",
    "                               DEFAULT_TEMPTED_PC,\n",
    "                               DEFAULT_TEMPTED_EP,\n",
    "                               DEFAULT_TEMPTED_SMTH,\n",
    "                               DEFAULT_TEMPTED_RES,\n",
    "                               DEFAULT_TEMPTED_MAXITER,\n",
    "                               DEFAULT_TEMPTED_RH as DEFAULT_TRH,\n",
    "                               DEFAULT_TEMPTED_RHC as DEFAULT_RC,\n",
    "                               DEFAULT_TEMPTED_SVDC,\n",
    "                               DEFAULT_TEMPTED_SVDCN as DEFAULT_TSCN)\n",
    "\n",
    "from gemelli.tempted import (freg_rkhs, bernoulli_kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def udpate_residuals(table_mods, a_hat, b_hats,\n",
    "                     phi_hats, times, lambdas):\n",
    "    '''\n",
    "    Update the tensor to be factorized by subtracting the \n",
    "    approximation the previous iteration. In other words,\n",
    "    calculate the residuals.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    table_mods: dictionary, required\n",
    "        Tables for each modality\n",
    "        keys = modality\n",
    "        values = DataFrame\n",
    "            rows = features\n",
    "            columns = samples\n",
    "    \n",
    "    a_hat: np.narray, required\n",
    "        Subject loadings\n",
    "    \n",
    "    b_hats: dictionary, required\n",
    "        Feature loadings\n",
    "        keys = modality\n",
    "        values = loadings\n",
    "    \n",
    "    phi_hats: dictionary, required\n",
    "        Temporal loadings\n",
    "        keys = modality\n",
    "        values = loadings\n",
    "    \n",
    "    times: dictionary, required\n",
    "        Time points for each modality\n",
    "        keys = modality\n",
    "        values = list of numpy.ndarray\n",
    "            list[0] = time points within interval\n",
    "            list[1] = individual indexes\n",
    "    \n",
    "    lambdas: dictionary, required\n",
    "        Singular values\n",
    "        keys = modality\n",
    "        values = loadings\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    tables_update: dictionary\n",
    "        Residuals for each modality\n",
    "        keys = modality\n",
    "        values = DataFrame\n",
    "            rows = features\n",
    "            columns = samples\n",
    "\n",
    "    Raises\n",
    "    ----------\n",
    "    TODO\n",
    "    '''\n",
    "    \n",
    "    tables_update = copy.deepcopy(table_mods)\n",
    "\n",
    "    for modality in tables_update.keys():\n",
    "\n",
    "        #get key modality-specific variables\n",
    "        table_mod = tables_update[modality]\n",
    "        b_hat = b_hats[modality]\n",
    "        phi_hat = phi_hats[modality]\n",
    "        ti = times[modality][0]\n",
    "        lambda_coeff = lambdas[modality]\n",
    "    \n",
    "        for i, (individual_id, m) in enumerate(table_mod.items()):\n",
    "            phi_ = phi_hat[ti[i]]\n",
    "            scale_tmp = b_hat.dot(phi_)\n",
    "            scale_tmp = a_hat * scale_tmp\n",
    "            table_mod[individual_id] -= (lambda_coeff * scale_tmp)\n",
    "\n",
    "        tables_update[modality] = table_mod\n",
    "\n",
    "    return tables_update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_lambda(individual_id_tables, ti, \n",
    "                  a_hat, phi_hat, b_hat):\n",
    "    '''\n",
    "    Updates the singular values using the loadings\n",
    "    from the most recent iteration\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    individual_id_tables: dictionary, required\n",
    "        Dictionary of tables constructed. Note that at this point\n",
    "        the tables have been subset to only include the time points\n",
    "        within the previously defined interval.\n",
    "        keys = individual_ids\n",
    "        values = DataFrame, required\n",
    "            rows = features\n",
    "            columns = samples\n",
    "    ti: list of int, required\n",
    "        Time points within predefined interval for\n",
    "        each individual \n",
    "    a_hat: np.narray, required\n",
    "        Subject loadings from the previous iteration\n",
    "    phi_hats: np.narray, required\n",
    "        Temporal loadings from the previous iteration\n",
    "    b_hat: np.narray, required\n",
    "        Feature loadings from the previous iteration\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    lambda_new: dictionary\n",
    "        Updated singular values\n",
    "        keys = modality\n",
    "        values = loadings\n",
    "        \n",
    "    Raises\n",
    "    ----------\n",
    "    TODO\n",
    "    '''\n",
    "    \n",
    "    nums = []\n",
    "    denoms = []\n",
    "\n",
    "    for i, m in enumerate(individual_id_tables.values()):\n",
    "        \n",
    "        phi_ = phi_hat[ti[i]]\n",
    "        num = a_hat[i]*(b_hat.dot(m.values).dot(phi_))\n",
    "        nums.append(num)\n",
    "        denom = (a_hat[i]*phi_) ** 2\n",
    "        denom = np.sum(denom)\n",
    "        denoms.append(denom)\n",
    "    \n",
    "    lambda_new = np.sum(nums) / np.sum(denoms)\n",
    "\n",
    "    return lambda_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_a_mod(individual_id_tables, \n",
    "                 n_individuals, n_features,\n",
    "                 b_mod, phi_mod, \n",
    "                 lambda_mod, ti):\n",
    "    '''\n",
    "    Update the tabular loadings (subjects and features) loadings\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    individual_id_tables: dictionary, required\n",
    "        Dictionary of tables constructed. Note that at this point\n",
    "        the tables have been subset to only include the time points\n",
    "        within the previously defined interval.\n",
    "        keys = individual_ids\n",
    "        values = DataFrame, required\n",
    "            rows = features\n",
    "            columns = samples\n",
    "\n",
    "    n_individuals: int, required\n",
    "        Number of unique individuals/samples in modality\n",
    "\n",
    "    n_features: int, required\n",
    "        Number of unique features in modality\n",
    "\n",
    "    b_mod: np.narray, required\n",
    "        Feature loadings from a specific modality\n",
    "\n",
    "    phi_mod: np.narray, required\n",
    "        Temporal loadings from a specific modality\n",
    "\n",
    "    lambda_mod: float, required\n",
    "        Singular value from a specific modality\n",
    "\n",
    "    ti: list of int, required\n",
    "        Time points within predefined interval for\n",
    "        each individual\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    a_new: np.narray\n",
    "        Updated subject loadings\n",
    "    b_new: np.narray\n",
    "        Updated feature loadings\n",
    "        \n",
    "    Raises\n",
    "    ----------\n",
    "    TODO\n",
    "    '''\n",
    "\n",
    "    #initialize intermediate outputs\n",
    "    a_num = {}\n",
    "    a_denom = {}\n",
    "    b_num = np.zeros((n_features, n_individuals))\n",
    "    common_denom = {}\n",
    "\n",
    "    for i, (individual_id, m) in enumerate(individual_id_tables.items()):\n",
    "\n",
    "        #keep only relevant timepoints (within interval)\n",
    "        phi_ = phi_mod[ti[i]]\n",
    "        #save item needed for both a_hat and b_hat\n",
    "        common_denom[individual_id] = np.sum(phi_ ** 2)\n",
    "        #save item needed later for b_hat\n",
    "        b_num[:, i] = (m.values).dot(phi_)  #vector per individual\n",
    "        #a_hat specific operations\n",
    "        a_num_mod = lambda_mod*b_mod.dot(m.values).dot(phi_)\n",
    "        a_num[individual_id] = a_num_mod\n",
    "        a_denom[individual_id] = (lambda_mod ** 2)*common_denom[individual_id]\n",
    "\n",
    "    return a_num, a_denom, b_num, common_denom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_tabular(individual_id_tables, \n",
    "                       n_individuals,\n",
    "                       n_components=3):\n",
    "                   \n",
    "    \"\"\"\n",
    "    Initialize subject and feature loadings\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    individual_id_tables: dictionary, required\n",
    "        Dictionary of tables constructed.\n",
    "        (see build_sparse class)\n",
    "        keys = individual_ids\n",
    "        values = DataFrame, required\n",
    "            rows = features\n",
    "            columns = samples\n",
    "\n",
    "    n_individuals: int, required\n",
    "        Number of unique individuals/samples\n",
    "\n",
    "    n_components: int, optional : Default is 3\n",
    "        The underlying rank of the data and number of\n",
    "        output dimentions.\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    b_hat: dictionary\n",
    "        Updated feature loadings\n",
    "        keys = modality\n",
    "        values = loadings\n",
    "        \n",
    "    Raises\n",
    "    ----------\n",
    "    TODO\n",
    "    \"\"\"\n",
    "\n",
    "    # initialize feature loadings\n",
    "    data_unfold = np.hstack([m for m in individual_id_tables.values()])\n",
    "    u, e, v = svds(data_unfold, k=n_components, which='LM')\n",
    "    u, e, v = svd_sort(u, np.diag(e), v)\n",
    "    b_hat = u[:, 0]\n",
    "\n",
    "    # initialize subject loadings\n",
    "    consistent_sign = np.sign(np.sum(b_hat))\n",
    "    a_hat = (np.ones(n_individuals) / np.sqrt(n_individuals)) * consistent_sign\n",
    "\n",
    "    return b_hat, a_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decomposition_iter(table_mods, times, \n",
    "                       individual_id_lst, \n",
    "                       Kmats, Kmat_outputs,\n",
    "                       maxiter, epsilon,\n",
    "                       smooth, n_components):\n",
    "    '''\n",
    "    Iterate over the available modalities\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    table_mods: dictionary, required\n",
    "        Updated tables for each modality. Times are\n",
    "        normalized and only points within the interval\n",
    "        are kept.\n",
    "        keys = modality\n",
    "        values = DataFrame\n",
    "            rows = features\n",
    "            columns = samples\n",
    "\n",
    "    individual_id_lst: list, required\n",
    "        List of unique individual IDs\n",
    "\n",
    "    times: dictionary, required\n",
    "        Updated time points for each modality\n",
    "        keys = modality\n",
    "        values = list of numpy.ndarray\n",
    "            list[0] = time points within interval\n",
    "            list[1] = individual indexes\n",
    "\n",
    "    Kmats: dictionary, required\n",
    "        Kernel matrix for each modality\n",
    "        keys = modality\n",
    "        values = numpy.ndarray\n",
    "            rows, columns = time points\n",
    "    \n",
    "    Kmat_outputs: dictionary, required\n",
    "        Bernoulli kernel matrix for each modality\n",
    "        keys = modality\n",
    "        values = numpy.ndarray\n",
    "            rows = resolution\n",
    "            columns = time points\n",
    "\n",
    "    maxiter: int, optional : Default is 20\n",
    "        Maximum number of iteration in for rank-1 calculation\n",
    "\n",
    "    epsilon: float, optional : Default is 0.0001\n",
    "        Convergence criteria for difference between iterations\n",
    "        for each rank-1 calculation.\n",
    "\n",
    "    smooth: float, optional : Default is 1e-6\n",
    "        Smoothing parameter for the kernel matrix\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    Rank-1 loadings\n",
    "    a_hat: np.narray\n",
    "        Subject loadings, shared across modalities\n",
    "\n",
    "    b_hats: dictionary\n",
    "        Feature loadings\n",
    "        keys = modality\n",
    "        values = loadings\n",
    "\n",
    "    phi_hats: dictionary\n",
    "        Temporal loadings\n",
    "        keys = modality\n",
    "        values = loadings\n",
    "\n",
    "    lambdas: dictionary\n",
    "        Singular values\n",
    "        keys = modality\n",
    "        values = loadings\n",
    "        \n",
    "    Raises\n",
    "    ----------\n",
    "    TODO\n",
    "    '''\n",
    "\n",
    "    a_hats = {}\n",
    "    b_hats = {}\n",
    "    phi_hats = {}\n",
    "    lambdas = {}\n",
    "    common_denom = {}\n",
    "    b_num = {}\n",
    "    \n",
    "    #iterate until convergence\n",
    "    t = 0\n",
    "    dif = 1\n",
    "    while t <= maxiter and dif > epsilon:            \n",
    "        \n",
    "        #variables to save intermediate outputs\n",
    "        a_num = {}\n",
    "        a_denom = {}\n",
    "        b_hat_difs = {}\n",
    "        for modality in table_mods.keys():\n",
    "            \n",
    "            #get key modality-specific variables\n",
    "            table_mod = table_mods[modality]\n",
    "            ti, ind_vec = times[modality]\n",
    "            Kmat = Kmats[modality]\n",
    "            Kmat_output = Kmat_outputs[modality]\n",
    "            n_individuals = len(table_mod)\n",
    "            first_ind = list(table_mod.keys())[0]\n",
    "            n_features = table_mod[first_ind].shape[0]\n",
    "            \n",
    "            if t == 0:\n",
    "                print('Initializing a_hat and b_hat')\n",
    "                #initialize feature and subject loadings\n",
    "                b_hat, a_hat = initialize_tabular(table_mod, \n",
    "                                                  n_individuals=n_individuals,\n",
    "                                                  n_components=n_components)\n",
    "                b_hats[modality] = b_hat\n",
    "                a_hats[modality] = a_hat\n",
    "            if t > 0:\n",
    "                #update feature loadings\n",
    "                b_temp = b_num[modality]\n",
    "                #print(common_denom[modality])\n",
    "                b_new = np.dot(b_temp, a_hat) / np.dot(common_denom[modality], a_hat ** 2)\n",
    "                b_hat = b_new / np.sqrt(np.sum(b_new ** 2))\n",
    "                b_hat_difs[modality] = np.sum((b_hats[modality] - b_hat) ** 2)\n",
    "                b_hats[modality] = b_hat\n",
    "            \n",
    "            #calculate state loadings\n",
    "            Ly = [a_hat[i] * b_hat.dot(m) for i, m in enumerate(table_mod.values())]\n",
    "            phi_hat = freg_rkhs(Ly, a_hat, ind_vec, Kmat, Kmat_output, smooth=smooth)\n",
    "            phi_hat = (phi_hat / np.sqrt(np.sum(phi_hat ** 2)))\n",
    "            phi_hats[modality] = phi_hat\n",
    "            #calculate lambda\n",
    "            lambda_mod = update_lambda(table_mod, ti, a_hat, phi_hat, b_hat)\n",
    "            lambdas[modality] = lambda_mod\n",
    "            #begin updating subject and feature loadings\n",
    "            (a_mod_num, a_mod_denom, \n",
    "             b_mod_num, common_mod_denom) = update_a_mod(table_mod, n_individuals, n_features,\n",
    "                                                         b_hat, phi_hat, lambda_mod, ti)\n",
    "            #save intermediate b-hat variables\n",
    "            b_num[modality] = b_mod_num\n",
    "            common_denom[modality] = common_mod_denom\n",
    "            #add subject loading variables\n",
    "            a_num = {**a_num, **{key: a_mod_num[key] + a_num.get(key, 0) \n",
    "                                 for key in a_mod_num}}\n",
    "            a_denom = {**a_denom, **{key: a_mod_denom[key] + a_denom.get(key, 0) \n",
    "                                     for key in a_mod_denom}}\n",
    "        #update subject loadings\n",
    "        a_tilde = np.array([a_num[id] / a_denom[id] for id in individual_id_lst])\n",
    "        a_scaling = np.sqrt(np.sum(a_tilde ** 2))\n",
    "        a_new = np.array([a_tilde[i] / a_scaling for i in range(len(a_tilde))])\n",
    "        a_hat_dif = np.sum((a_hat - a_new) ** 2)\n",
    "        a_hat = a_new\n",
    "        #check for convergence\n",
    "        dif = max([a_hat_dif]+list(b_hat_difs.values())) #or take mean of b_hat_difs?    \n",
    "        t += 1\n",
    "    print(\"Reached convergence in {} iterations\".format(t))\n",
    "\n",
    "    return a_hat, b_hats, phi_hats, lambdas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_time(individual_id_tables,\n",
    "                individual_id_state_orders, \n",
    "                n_individuals, resolution, \n",
    "                input_time_range, interval=None):\n",
    "    '''\n",
    "    Normalize time points to be in the same format and keep\n",
    "    only the defined interval (if defined)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    individual_id_tables: dictionary, required\n",
    "        Dictionary of tables constructed.\n",
    "        (see build_sparse class)\n",
    "        keys = individual_ids\n",
    "        values = DataFrame, required\n",
    "            rows = features\n",
    "            columns = samples\n",
    "\n",
    "    individual_id_state_orders : dict\n",
    "        Dictionary of time points for each individual\n",
    "\n",
    "    n_individuals: int, required\n",
    "        Number of unique individuals/samples\n",
    "\n",
    "    resolution: int, optional : Default is 101\n",
    "        Number of time points to evaluate the value\n",
    "        of the temporal loading function.\n",
    "\n",
    "    input_time_range: tuple, required\n",
    "        Start and end time points for each individual\n",
    "\n",
    "    interval : tuple, optional\n",
    "        Start and end time points to keep\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    ti: list of numpy.ndarray\n",
    "        List of time points within defined interval \n",
    "        per subject\n",
    "\n",
    "    ind_vec: numpy.ndarray\n",
    "        Subject indexes for each time point\n",
    "\n",
    "    tm: numpy.ndarray\n",
    "        Concatenated normalized time points for all\n",
    "        subjects\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    TODO\n",
    "    '''\n",
    "\n",
    "    # make copy of tables to update\n",
    "    tables_update = copy.deepcopy(individual_id_tables)\n",
    "    orders_update = copy.deepcopy(individual_id_state_orders)\n",
    "    \n",
    "    # normalize time points\n",
    "    for individual_id in orders_update.keys():\n",
    "        orders_update[individual_id] = (orders_update[individual_id] - input_time_range[0]) \\\n",
    "                                        / (input_time_range[1] - input_time_range[0])\n",
    "    # ensure interval is in the same format\n",
    "    interval = tuple((interval - input_time_range[0]) \\\n",
    "                     / (input_time_range[1] - input_time_range[0]))\n",
    "    \n",
    "    # initialize variables to store time points (tps)\n",
    "    Lt = [] # all normalized tps\n",
    "    ind_vec = [] #individual indexes for each tp\n",
    "    ti = [[] for i in range(n_individuals)] # tps within interval per subject\n",
    "    \n",
    "    # populate variables above\n",
    "    for i, (id_, time_range_i) in enumerate(orders_update.items()):\n",
    "        # save all normalized time points\n",
    "        Lt.append(time_range_i)\n",
    "        ind_vec.extend([i] * len(Lt[-1]))\n",
    "        # define time points within interval\n",
    "        mask = (time_range_i >= interval[0]) & (time_range_i <= interval[1])\n",
    "        temp = time_range_i[mask]\n",
    "        temp = [(resolution-1)*(tp - interval[0])/(interval[1] - interval[0]) for tp in temp]\n",
    "        ti[i] = np.array(list(map(int, temp)))\n",
    "        # update tables and orders\n",
    "        tables_update[id_] = tables_update[id_].T[mask].T\n",
    "    \n",
    "    # convert variables to numpy arrays \n",
    "    ind_vec = np.array(ind_vec)\n",
    "    tm = np.concatenate(Lt)\n",
    "\n",
    "    return interval, tables_update, ti, ind_vec, tm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatting_iter(individual_id_tables, \n",
    "                    individual_id_state_orders,\n",
    "                    mod_id_ind, input_time_range, \n",
    "                    interval, resolution):\n",
    "    '''\n",
    "    Format the input data for downstream tasks and \n",
    "    calculate tne kernel matrix.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    individual_id_tables: dictionary, required\n",
    "        Dictionary of 1 to n tables constructed,\n",
    "        (see build_sparse class), where n is the \n",
    "        number of modalities.\n",
    "        keys = individual_ids\n",
    "        values = list of DataFrame, required\n",
    "            For each DataFrame (modality):\n",
    "            rows = features\n",
    "            columns = samples\n",
    "\n",
    "    individual_id_state_orders: dictionary, required\n",
    "        Dictionary of 1 to n lists of time points (one \n",
    "        per modality) for each sample.\n",
    "        keys = individual_ids\n",
    "        values = list of numpy.ndarray\n",
    "            Each numpy.ndarray contains the time points\n",
    "            of the corresponding modality\n",
    "            Note: array of dtype=object to allow for\n",
    "            different number of time points per modality\n",
    "\n",
    "    mod_id_ind: dictionary, required\n",
    "        Dictionary of individual IDs for each modality\n",
    "        keys = modality\n",
    "        values = list of tuples\n",
    "            Each tuple contains the individual id and \n",
    "            the dataframe index in individual_id_tables\n",
    "\n",
    "    input_time_range: tuple, required\n",
    "        Start and end time points for each individual\n",
    "\n",
    "    interval : tuple, optional\n",
    "        Start and end time points to keep\n",
    "\n",
    "    resolution: int, optional : Default is 101\n",
    "        Number of time points to evaluate the value\n",
    "        of the temporal loading function.\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    table_mods: dictionary\n",
    "        Updated tables for each modality. Times are\n",
    "        normalized and only points within the interval\n",
    "        are kept.\n",
    "        keys = modality\n",
    "        values = DataFrame\n",
    "            rows = features\n",
    "            columns = samples\n",
    "\n",
    "    times: dictionary\n",
    "        Updated time points for each modality\n",
    "        keys = modality\n",
    "        values = list of numpy.ndarray\n",
    "            list[0] = time points within interval\n",
    "            list[1] = individual indexes\n",
    "\n",
    "    Kmats: dictionary\n",
    "        Kernel matrix for each modality\n",
    "        keys = modality\n",
    "        values = numpy.ndarray\n",
    "            rows, columns = time points\n",
    "    \n",
    "    Kmat_outputs: dictionary\n",
    "        Bernoulli kernel matrix for each modality\n",
    "        keys = modality\n",
    "        values = numpy.ndarray\n",
    "            rows = resolution\n",
    "            columns = time points\n",
    "\n",
    "    Raises\n",
    "    ----------\n",
    "    TODO\n",
    "    '''\n",
    "\n",
    "    #initialize dictionary to store outputs per modality\n",
    "    #To-do: save in self\n",
    "    table_mods = {}\n",
    "    times = {}\n",
    "    Kmats = {}\n",
    "    Kmat_outputs = {}\n",
    "    \n",
    "    #iterate through each modality\n",
    "    for modality in mod_id_ind.keys():        \n",
    "\n",
    "        #get the individual IDs\n",
    "        ind_tuple_lst = mod_id_ind[modality]\n",
    "        #keep modality-specific time points\n",
    "        orders_mod = {ind[0]: individual_id_state_orders[ind[0]][ind[1]] \n",
    "                      for ind in ind_tuple_lst}\n",
    "        #keep modality-specific tables\n",
    "        table_mod = {ind[0]: individual_id_tables[ind[0]][ind[1]] \n",
    "                     for ind in ind_tuple_lst}\n",
    "        n_individuals = len(table_mod)\n",
    "        #format time points and keep points in the interval\n",
    "        (norm_interval, table_mod, \n",
    "         ti, ind_vec, tm) = format_time(table_mod, orders_mod,\n",
    "                                        n_individuals, resolution,\n",
    "                                        input_time_range, interval)\n",
    "        #save key outputs\n",
    "        table_mods[modality] = table_mod\n",
    "        times[modality] = [ti, ind_vec]\n",
    "        #construct the kernel matrix\n",
    "        Kmats[modality] = bernoulli_kernel(tm, tm)\n",
    "        Kmat_outputs[modality] = bernoulli_kernel(np.linspace(norm_interval[0],\n",
    "                                                              norm_interval[1],\n",
    "                                                              num=resolution),\n",
    "                                                 tm)\n",
    "    \n",
    "    return table_mods, times, Kmats, Kmat_outputs, norm_interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def joint_ctf_helper(individual_id_tables,\n",
    "                     individual_id_state_orders,\n",
    "                     mod_id_ind, interval,\n",
    "                     resolution, maxiter,\n",
    "                     epsilon, smooth, \n",
    "                     n_components):\n",
    "    '''\n",
    "    Joint decomposition of two or more tensors\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    individual_id_tables: dictionary, required\n",
    "        Dictionary of 1 to n tables constructed,\n",
    "        (see build_sparse class), where n is the \n",
    "        number of modalities.\n",
    "        keys = individual_ids\n",
    "        values = list of DataFrame, required\n",
    "            For each DataFrame (modality):\n",
    "            rows = features\n",
    "            columns = samples\n",
    "\n",
    "    individual_id_state_orders: dictionary, required\n",
    "        Dictionary of 1 to n lists of time points (one \n",
    "        per modality) for each sample.\n",
    "        keys = individual_ids\n",
    "        values = list of numpy.ndarray\n",
    "            Each numpy.ndarray contains the time points\n",
    "            of the corresponding modality\n",
    "            Note: array of dtype=object to allow for\n",
    "            different number of time points per modality\n",
    "\n",
    "    mod_id_ind: dictionary, required\n",
    "        Dictionary of individual IDs for each modality\n",
    "        keys = modality\n",
    "        values = list of tuples\n",
    "            Each tuple contains the individual id and \n",
    "            the dataframe index in individual_id_tables\n",
    "\n",
    "    interval : tuple, optional\n",
    "        Start and end time points to keep\n",
    "\n",
    "    resolution: int, optional : Default is 101\n",
    "        Number of time points for the temporal \n",
    "        loading function.\n",
    "\n",
    "    maxiter: int, optional : Default is 20\n",
    "        Maximum number of iteration in for rank-1 calculation.\n",
    "\n",
    "    epsilon: float, optional : Default is 0.0001\n",
    "        Convergence criteria for difference between iterations\n",
    "        for each rank-1 calculation.\n",
    "\n",
    "    smooth: float, optional : Default is 1e-8\n",
    "        Smoothing parameter for RKHS norm. Larger means\n",
    "        smoother temporal loading functions.\n",
    "\n",
    "    n_components: int, optional : Default is 3\n",
    "        The underlying rank of the data and number of\n",
    "        output dimentions.\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    individual_loadings: pd.DataFrame\n",
    "        Subject loadings\n",
    "        rows = individual IDs\n",
    "        columns = component number\n",
    "    \n",
    "    feature_loadings: dictionary\n",
    "        Feature loadings\n",
    "        keys = component number\n",
    "        values = dictionary of modality-specific loadings\n",
    "    \n",
    "    state_loadings: dictionary\n",
    "        Temporal loadings\n",
    "\n",
    "    lambda_coeff: dictionary\n",
    "        Singular values\n",
    "\n",
    "    time_return: np.ndarray\n",
    "        Time points for the temporal loading function\n",
    "    '''\n",
    "    \n",
    "    #make copy of tables to update\n",
    "    tables_update = copy.deepcopy(individual_id_tables)\n",
    "    orders_update = copy.deepcopy(individual_id_state_orders)\n",
    "    #get all individual IDs\n",
    "    individual_id_lst = list(orders_update.keys())\n",
    "    n_individuals_all = len(individual_id_lst)\n",
    "    #get all time points across all modalities\n",
    "    timestamps_all = np.concatenate(list(orders_update.values()), axis=1)\n",
    "    timestamps_all = np.concatenate(timestamps_all)\n",
    "    timestamps_all = np.unique(timestamps_all)\n",
    "    # set the interval if none is given\n",
    "    if interval is None:\n",
    "        interval = (timestamps_all[0], timestamps_all[-1])\n",
    "    # set time ranges [0, 1]\n",
    "    input_time_range = (timestamps_all[0], timestamps_all[-1])\n",
    "\n",
    "    #format time points and keep points in defined interval\n",
    "    print('Formatting time points')\n",
    "    (table_mods, times,\n",
    "    Kmats, Kmat_outputs,\n",
    "    norm_interval) = formatting_iter(tables_update, \n",
    "                                     orders_update,\n",
    "                                     mod_id_ind, \n",
    "                                     input_time_range,\n",
    "                                     interval, resolution)\n",
    "    #init dataframes to fill\n",
    "    #key: component number, value: dictionary of modality-specific loadings\n",
    "    n_component_col_names = ['component_' + str(i+1)\n",
    "                             for i in range(n_components)]\n",
    "    individual_loadings = pd.DataFrame(np.zeros((n_individuals_all, n_components)),\n",
    "                                       index=tables_update.keys(),\n",
    "                                       columns=n_component_col_names)\n",
    "    feature_loadings = {}\n",
    "    state_loadings = {}\n",
    "    lambda_coeff = {} \n",
    "\n",
    "    #perform decomposition\n",
    "    for r in range(n_components):\n",
    "        comp_name = 'component_' + str(r+1)\n",
    "        print('Calculate components for {}'.format(comp_name))\n",
    "        (a_hat, b_hats, \n",
    "         phi_hats, lambdas) = decomposition_iter(table_mods, times,\n",
    "                                                 individual_id_lst,\n",
    "                                                 Kmats, Kmat_outputs,\n",
    "                                                 maxiter, epsilon, \n",
    "                                                 smooth, n_components)\n",
    "        #save rank-1 components\n",
    "        individual_loadings.iloc[:, r] = a_hat\n",
    "        feature_loadings[comp_name] = b_hats\n",
    "        state_loadings[comp_name] = phi_hats\n",
    "        lambda_coeff[comp_name] = lambdas\n",
    "\n",
    "        print('Calculate residuals and update tables')\n",
    "        #calculate residuals and update tables\n",
    "        tables_update = udpate_residuals(table_mods, a_hat, b_hats, \n",
    "                                         phi_hats, times, lambdas)\n",
    "        \n",
    "        table_mods = tables_update\n",
    "\n",
    "    #TODO: revise signs to make sure summation is non-negative(?)\n",
    "    #TODO: find better format to save decompositions(?)\n",
    "    #return original time points\n",
    "    time_return = np.linspace(norm_interval[0],\n",
    "                              norm_interval[1],\n",
    "                              resolution)\n",
    "    time_return *= (input_time_range[1] - input_time_range[0])\n",
    "    time_return += input_time_range[0]\n",
    "\n",
    "    return (individual_loadings, feature_loadings, \n",
    "            state_loadings, lambda_coeff, time_return)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "class concat_tensors():\n",
    "\n",
    "    '''\n",
    "    Concatenate the tensors from each modality into a\n",
    "    single tensor class\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    tensors: dictionary, required\n",
    "        Dictionary of tensors constructed.\n",
    "        keys = modality\n",
    "        values = tensor, required\n",
    "            rows = features\n",
    "            columns = samples\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    self: object\n",
    "        Returns the instance itself\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def concat(self, tensors):\n",
    "        '''\n",
    "        Concatenate tensors from each modality into a\n",
    "        single tensor. Note: tensors should have been\n",
    "        preprocessed by this point.\n",
    "        '''\n",
    "\n",
    "        individual_id_tables = {}\n",
    "        individual_id_state_orders = {}\n",
    "        mod_id_ind = {}\n",
    "        \n",
    "        for mod, tensor in tensors.items():\n",
    "            \n",
    "            #concatenate tables\n",
    "            for ind_id, table in tensor.individual_id_tables_centralized.items():\n",
    "                individual_id_tables[ind_id] = individual_id_tables.get(ind_id, []) + [table]\n",
    "                mod_id_ind[mod] = mod_id_ind.get(mod, []) + [(ind_id, len(individual_id_tables[ind_id])-1)]\n",
    "            #concatenate state orders\n",
    "            for ind_id, order in tensor.individual_id_state_orders.items():\n",
    "                individual_id_state_orders[ind_id] = individual_id_state_orders.get(ind_id, []) + [order]\n",
    "\n",
    "        ##TODO make sure individuals are ordered the same way in all dictionaries?\n",
    "        \n",
    "        #store all to self\n",
    "        self.individual_id_tables = individual_id_tables\n",
    "        self.individual_id_state_orders = individual_id_state_orders\n",
    "        self.mod_id_ind = mod_id_ind\n",
    "\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def joint_ctf(tables, \n",
    "              sample_metadatas,\n",
    "              modality_ids,\n",
    "              individual_id_column: str,\n",
    "              state_column: str,\n",
    "              interval: tuple = None,\n",
    "              n_components: int = DEFAULT_COMP,\n",
    "              ##could be done separately by user\n",
    "              ##also for rclr or other transformations\n",
    "              ##default can be same transformation\n",
    "              min_sample_count: int = DEFAULT_MSC,\n",
    "              min_feature_count: int = DEFAULT_MFC,\n",
    "              min_feature_frequency: float = DEFAULT_MFF,\n",
    "              transformation: Callable = matrix_rclr,\n",
    "              pseudo_count: float = DEFAULT_TEMPTED_PC,\n",
    "              ##important to test dif modalities\n",
    "              replicate_handling: str = DEFAULT_TRH,\n",
    "              svd_centralized: bool = DEFAULT_TEMPTED_SVDC,\n",
    "              n_components_centralize: int = DEFAULT_TSCN,\n",
    "              smooth: float = DEFAULT_TEMPTED_SMTH,\n",
    "              resolution: int = DEFAULT_TEMPTED_RES,\n",
    "              max_iterations: int = DEFAULT_TEMPTED_MAXITER,\n",
    "              epsilon: float = DEFAULT_TEMPTED_EP):\n",
    "    '''\n",
    "    Joint decomposition of two or more tensors\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    tables: list of numpy.ndarray, required\n",
    "        List of feature tables (1-n) from different modalities\n",
    "        in biom format containing the samples over which\n",
    "        metrics should be computed.\n",
    "        Each modality should contain same number of samples\n",
    "        or individuals. Length of features might vary.\n",
    "    \n",
    "    sample_metadatas: list of DataFrame, required\n",
    "        Sample metadata files in QIIME2 formatting for each \n",
    "        modality. The file must contain the columns for \n",
    "        individual_id_column and state_column and the rows\n",
    "        matched to the table.\n",
    "\n",
    "    modality_ids: list of str, required\n",
    "        Unique identifier for each modality.\n",
    "\n",
    "    individual_id_column: str, required\n",
    "        Metadata column containing subject IDs to use for\n",
    "        pairing samples. WARNING: if replicates exist for an\n",
    "        individual ID at either state_1 to state_N, that\n",
    "        subject will be mean grouped by default.\n",
    "\n",
    "    state_column: str, required\n",
    "        Metadata column containing state (e.g.,Time,\n",
    "        BodySite) across which samples are paired. At least\n",
    "        one is required but up to four are allowed by other\n",
    "        state inputs.\n",
    "\n",
    "    interval: tuple, optional : Default is None\n",
    "        Start and end time points to keep.\n",
    "\n",
    "    n_components: int, optional : Default is 3\n",
    "        The underlying rank of the data and number of\n",
    "        output dimentions.\n",
    "\n",
    "    min_sample_count: int, optional : Default is 0\n",
    "        Minimum sum cutoff of sample across all features.\n",
    "        The value can be at minimum zero and must be an\n",
    "        whole integer. It is suggested to be greater than\n",
    "        or equal to 500.\n",
    "\n",
    "    min_feature_count: int, optional : Default is 0\n",
    "        Minimum sum cutoff of features across all samples.\n",
    "        The value can be at minimum zero and must be\n",
    "        an whole integer.\n",
    "\n",
    "    min_feature_frequency: float, optional : Default is 0\n",
    "        Minimum percentage of samples a feature must appear\n",
    "        with a value greater than zero. This value can range\n",
    "        from 0 to 100 with decimal values allowed.\n",
    "\n",
    "    transformation: function, optional : Default is matrix_rclr\n",
    "        The transformation function to use on the data.\n",
    "\n",
    "    pseudo_count: float, optional : Default is 1\n",
    "        The pseudo count to add to all values before applying\n",
    "        the transformation.\n",
    "\n",
    "    replicate_handling: function, optional : Default is \"sum\"\n",
    "        Choose how replicate samples are handled. If replicates are\n",
    "        detected, \"error\" causes method to fail; \"drop\" will discard\n",
    "        all replicated samples; \"random\" chooses one representative at\n",
    "        random from among replicates.\n",
    "\n",
    "    svd_centralized: bool, optional : Default is True\n",
    "        Removes the mean structure of the temporal tensor.\n",
    "\n",
    "    n_components_centralize: int\n",
    "        Rank of approximation for average matrix in svd-centralize.\n",
    "\n",
    "    smooth: float, optional : Default is 1e-8\n",
    "        Smoothing parameter for RKHS norm. Larger means\n",
    "        smoother temporal loading functions.\n",
    "\n",
    "    resolution: int, optional : Default is 101\n",
    "        Number of time points to evaluate the value\n",
    "        of the temporal loading function.\n",
    "\n",
    "    max_iterations: int, optional : Default is 20\n",
    "        Maximum number of iteration in for rank-1 calculation.\n",
    "\n",
    "    epsilon: float, optional : Default is 0.0001\n",
    "        Convergence criteria for difference between iterations\n",
    "        for each rank-1 calculation.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    OrdinationResults - TODO\n",
    "        Compositional biplot of subjects as points and\n",
    "        features as arrows. Where the variation between\n",
    "        subject groupings is explained by the log-ratio\n",
    "        between opposing arrows.\n",
    "\n",
    "    DataFrame - TODO\n",
    "        Each components temporal loadings across the\n",
    "        input resolution included as a column called\n",
    "        'time_interval'.\n",
    "\n",
    "    DistanceMatrix - TODO\n",
    "        A subject-subject distance matrix generated\n",
    "        from the euclidean distance of the\n",
    "        subject ordinations and itself.\n",
    "\n",
    "    DataFrame - TODO\n",
    "        The loadings from the SVD centralize\n",
    "        function, used for projecting new data.\n",
    "        Warning: If SVD-centering is not used\n",
    "        then the function will add all ones as the\n",
    "        output to avoid variable outputs.\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    ValueError\n",
    "        if features don't match between tables\n",
    "        across the values of the dictionary\n",
    "    ValueError\n",
    "        if id_ not in mapping\n",
    "    ValueError\n",
    "        if any state_column not in mapping\n",
    "    ValueError\n",
    "        Table is not 2-dimensional\n",
    "    ValueError\n",
    "        Table contains negative values\n",
    "    ValueError\n",
    "        Table contains np.inf or -np.inf\n",
    "    ValueError\n",
    "        Table contains np.nan or missing.\n",
    "    Warning\n",
    "        If a conditional-sample pair\n",
    "        has multiple IDs associated\n",
    "        with it. In this case the\n",
    "        default method is to mean them.\n",
    "    ValueError\n",
    "        `ValueError: n_components must be at least 2`.\n",
    "    ValueError\n",
    "        `ValueError: Data-table contains\n",
    "         either np.inf or -np.inf`.\n",
    "    ValueError\n",
    "        `ValueError: The n_components must be less\n",
    "         than the minimum shape of the input tensor`.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    TODO\n",
    "    '''\n",
    "    \n",
    "    #note: we assume each modality has a dif table and associated\n",
    "    #metadata. We also assume filtering conditions are the same\n",
    "    tensors = {}\n",
    "    for table, sample_metadata, mod_id in zip(tables, \n",
    "                                               sample_metadatas,\n",
    "                                               modality_ids):\n",
    "        print('Processing table for modality:', mod_id)\n",
    "        # check the table for validity and then filter\n",
    "        process_results = ctf_table_processing(table,\n",
    "                                               sample_metadata,\n",
    "                                               individual_id_column,\n",
    "                                               [state_column],\n",
    "                                               min_sample_count,\n",
    "                                               min_feature_count,\n",
    "                                               min_feature_frequency,\n",
    "                                               None)\n",
    "        table = process_results[0]\n",
    "        sample_metadata = process_results[1]\n",
    "\n",
    "        # build the sparse tensor format\n",
    "        print('Building sparse tensor for modality:', mod_id)\n",
    "        tensor = build_sparse()\n",
    "        tensor.construct(table,\n",
    "                        sample_metadata,\n",
    "                        individual_id_column,\n",
    "                        state_column,\n",
    "                        transformation=transformation,\n",
    "                        pseudo_count=pseudo_count,\n",
    "                        branch_lengths=None,\n",
    "                        replicate_handling=replicate_handling,\n",
    "                        svd_centralized=svd_centralized,\n",
    "                        n_components_centralize=n_components_centralize)\n",
    "        tensors[mod_id] = tensor\n",
    "    #save all tensors to a class\n",
    "    print('Concatenating all tensors')\n",
    "    n_tensors = concat_tensors().concat(tensors)\n",
    "\n",
    "    ##sanity checks\n",
    "    #print(n_tensors.individual_id_tables)\n",
    "    #print(n_tensors.individual_id_state_orders)\n",
    "    #print(n_tensors.mod_id_ind)\n",
    "    #return n_tensors.individual_id_state_orders\n",
    "\n",
    "    # run joint-CTF\n",
    "    print('Starting joint-ctf helper')\n",
    "    joint_ctf_res = joint_ctf_helper(n_tensors.individual_id_tables,\n",
    "                                     n_tensors.individual_id_state_orders,\n",
    "                                     n_tensors.mod_id_ind,\n",
    "                                     interval=interval,\n",
    "                                     resolution=resolution,\n",
    "                                     maxiter=max_iterations,\n",
    "                                     epsilon=epsilon,\n",
    "                                     smooth=smooth,\n",
    "                                     n_components=n_components)\n",
    "    (individual_loadings,\n",
    "     feature_loadings,\n",
    "     state_loadings,\n",
    "     eigenvalues,\n",
    "     time_return) = joint_ctf_res\n",
    "\n",
    "    return (individual_loadings, feature_loadings,\n",
    "            state_loadings, eigenvalues, time_return)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from biom import load_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../ipynb/tutorials/'\n",
    "# import table(s)\n",
    "table = load_table('{}IBD-2538/data/table.biom'.format(data_path))\n",
    "# import metadata\n",
    "metadata = pd.read_csv('{}IBD-2538/data/metadata.tsv'.format(data_path), sep='\\t',\n",
    "                       dtype={'sample_name':'str'}).set_index('sample_name')\n",
    "# import taxonomy\n",
    "taxonomy = pd.read_csv('{}IBD-2538/data/taxonomy.tsv'.format(data_path), sep='\\t',\n",
    "                       index_col=0, dtype={'sample_name':'str'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "individual_sample_number\n",
      "sample_type\n",
      "site_sampled\n"
     ]
    }
   ],
   "source": [
    "for col in metadata.columns:\n",
    "    if 'sample' in col:\n",
    "        print(col)\n",
    "\n",
    "#time: timepoint\n",
    "#subject: host_subject_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing table for modality: 16S\n",
      "Building sparse tensor for modality: 16S\n",
      "Concatenating all tensors\n"
     ]
    }
   ],
   "source": [
    "order_dict = joint_ctf(tables=[table], sample_metadatas=[metadata],\n",
    "                       modality_ids=['16S'], individual_id_column='host_subject_id',\n",
    "                       state_column='timepoint')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing table for modality: 16S\n",
      "Building sparse tensor for modality: 16S\n",
      "Concatenating all tensors\n",
      "Starting joint-ctf helper\n",
      "Formatting time points\n",
      "Calculate components for component_1\n",
      "Initializing a_hat and b_hat\n",
      "{'s1000100': 0.2511495884576869, 's1000200': 0.22325333302231923, 's1000300': 0.21964551286111547, 's1000500': 0.22824754561438976, 's1000600': 0.23974178734827234, 's1000700': 0.1710725399801568, 's1000900': 0.2511495884576869, 's1001000': 0.2511495884576869, 's1001100': 0.2511495884576869, 's1001300': 0.2511495884576869, 's1001400': 0.2511495884576869, 's1001500': 0.21821047624164192, 's1002000': 0.2201045368302758, 's1002100': 0.247467327558337, 's1002300': 0.17587242486273322, 's1002400': 0.2511495884576869, 's1002500': 0.13954490089497912, 's1002600': 0.1695773017395708, 's1002700': 0.15050554631351598, 's1003100': 0.1502417314319003, 's1003300': 0.23552286545726667, 's1003400': 0.07201830947641306, 's1003500': 0.15525152922592828, 's1003600': 0.13133025345507718}\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for *: 'dict' and 'float'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[180], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m (individual_loadings, feature_loadings,\n\u001b[0;32m----> 2\u001b[0m state_loadings, eigenvalues, time_return) \u001b[38;5;241m=\u001b[39m \u001b[43mjoint_ctf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtables\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mtable\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                                                      \u001b[49m\u001b[43msample_metadatas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                                                      \u001b[49m\u001b[43mmodality_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m16S\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                                                      \u001b[49m\u001b[43mindividual_id_column\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhost_subject_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                                                      \u001b[49m\u001b[43mstate_column\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtimepoint\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[134], line 220\u001b[0m, in \u001b[0;36mjoint_ctf\u001b[0;34m(tables, sample_metadatas, modality_ids, individual_id_column, state_column, interval, n_components, min_sample_count, min_feature_count, min_feature_frequency, transformation, pseudo_count, replicate_handling, svd_centralized, n_components_centralize, smooth, resolution, max_iterations, epsilon)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;66;03m##sanity checks\u001b[39;00m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;66;03m#print(n_tensors.individual_id_tables)\u001b[39;00m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;66;03m#print(n_tensors.individual_id_state_orders)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    217\u001b[0m \n\u001b[1;32m    218\u001b[0m \u001b[38;5;66;03m# run joint-CTF\u001b[39;00m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStarting joint-ctf helper\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 220\u001b[0m joint_ctf_res \u001b[38;5;241m=\u001b[39m \u001b[43mjoint_ctf_helper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_tensors\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindividual_id_tables\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mn_tensors\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindividual_id_state_orders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mn_tensors\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmod_id_ind\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43minterval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mresolution\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresolution\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mmaxiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_iterations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepsilon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43msmooth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msmooth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mn_components\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_components\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    229\u001b[0m (individual_loadings,\n\u001b[1;32m    230\u001b[0m  feature_loadings,\n\u001b[1;32m    231\u001b[0m  state_loadings,\n\u001b[1;32m    232\u001b[0m  eigenvalues,\n\u001b[1;32m    233\u001b[0m  time_return) \u001b[38;5;241m=\u001b[39m joint_ctf_res\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (individual_loadings, feature_loadings,\n\u001b[1;32m    236\u001b[0m         state_loadings, eigenvalues, time_return)\n",
      "Cell \u001b[0;32mIn[146], line 124\u001b[0m, in \u001b[0;36mjoint_ctf_helper\u001b[0;34m(individual_id_tables, individual_id_state_orders, mod_id_ind, interval, resolution, maxiter, epsilon, smooth, n_components)\u001b[0m\n\u001b[1;32m    121\u001b[0m comp_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcomponent_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(r\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCalculate components for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(comp_name))\n\u001b[1;32m    123\u001b[0m (a_hat, b_hats, \n\u001b[0;32m--> 124\u001b[0m  phi_hats, lambdas) \u001b[38;5;241m=\u001b[39m \u001b[43mdecomposition_iter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtable_mods\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43mindividual_id_lst\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43mKmats\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mKmat_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43mmaxiter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43msmooth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_components\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;66;03m#save rank-1 components\u001b[39;00m\n\u001b[1;32m    130\u001b[0m individual_loadings\u001b[38;5;241m.\u001b[39miloc[:, r] \u001b[38;5;241m=\u001b[39m a_hat\n",
      "Cell \u001b[0;32mIn[179], line 118\u001b[0m, in \u001b[0;36mdecomposition_iter\u001b[0;34m(table_mods, times, individual_id_lst, Kmats, Kmat_outputs, maxiter, epsilon, smooth, n_components)\u001b[0m\n\u001b[1;32m    116\u001b[0m b_temp \u001b[38;5;241m=\u001b[39m b_num[modality]\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28mprint\u001b[39m(common_denom[modality])\n\u001b[0;32m--> 118\u001b[0m b_new \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(b_temp, a_hat) \u001b[38;5;241m/\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommon_denom\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmodality\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma_hat\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m b_hat \u001b[38;5;241m=\u001b[39m b_new \u001b[38;5;241m/\u001b[39m np\u001b[38;5;241m.\u001b[39msqrt(np\u001b[38;5;241m.\u001b[39msum(b_new \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m))\n\u001b[1;32m    120\u001b[0m b_hat_difs[modality] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum((b_hats[modality] \u001b[38;5;241m-\u001b[39m b_hat) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for *: 'dict' and 'float'"
     ]
    }
   ],
   "source": [
    "(individual_loadings, feature_loadings,\n",
    "state_loadings, eigenvalues, time_return) = joint_ctf(tables=[table], \n",
    "                                                      sample_metadatas=[metadata],\n",
    "                                                      modality_ids=['16S'],\n",
    "                                                      individual_id_column='host_subject_id',\n",
    "                                                      state_column='timepoint')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sanity checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dummy orders_update\n",
    "individual_id_state_orders = {'ind1': np.array([[0, 0.5, 1, 2],[0, 0.5, 1],[0, 0.5, 1]], dtype=object),\n",
    "                              'ind2': np.array([[0, 1, 3],[0, 0.5, 1, 3]], dtype=object)}\n",
    "\n",
    "individual_id_state_orders2 = {'ind1': np.array([0, 0.5, 1, 2]),\n",
    "                              'ind2': np.array([0, 1, 3])}\n",
    "\n",
    "print(individual_id_state_orders)\n",
    "print(individual_id_state_orders2)\n",
    "\n",
    "#create random tables\n",
    "table1 = np.random.randint(0,100,size=(10, 4))\n",
    "table2 = np.random.randint(0,100,size=(12, 4))\n",
    "\n",
    "table3 = np.random.randint(0,100,size=(10, 3))\n",
    "table4 = np.random.randint(0,100,size=(12, 4))\n",
    "\n",
    "#create dictionary of tables\n",
    "#individual_id_tables = {'ind1': [table1, table2], 'ind2': [table3, table4]}\n",
    "#individual_id_mod = {'ind1': table1, 'ind2': table3}\n",
    "\n",
    "tensor1_tables = {'ind1': table1, 'ind2': table2, 'ind4': table1}\n",
    "tensor2_tables = {'ind1': table3, 'ind2': table4, 'ind3': table3}\n",
    "\n",
    "tensor1_state_orders = {'ind1': [0, 0.5, 1, 2], 'ind2': [0, 0.5, 1, 3], 'ind4': [0, 0.5, 1, 3]}\n",
    "tensor2_state_orders = {'ind1': [0, 0.5, 1], 'ind2': [0, 0.5, 1, 3], 'ind3': [0, 0.5, 2]}\n",
    "\n",
    "individual_id_orders = {}\n",
    "individual_id_tables = {}\n",
    "\n",
    "#concat lst from both tensors by individual\n",
    "for tensor in [tensor1_state_orders, tensor2_state_orders]:\n",
    "\n",
    "    for key, value in tensor.items():\n",
    "        individual_id_orders[key] = individual_id_orders.get(key, []) + [value]\n",
    "\n",
    "for tensor in [tensor1_tables, tensor2_tables]:\n",
    "\n",
    "    for key, value in tensor.items():\n",
    "        individual_id_tables[key] = individual_id_tables.get(key, []) + [value]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
